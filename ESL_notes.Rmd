---
title: "ESL_notes"
author: "Ben DeVries"
date: "2024-09-21"
header-includes:
    - \usepackage{bm}
output: html_document
---

**CHAPTER 8**

**Expectation Maximization for Gaussian Mixture** (272)

$Y_1\sim N(\mu_1,\sigma_1^2)$, $Y_2\sim N(\mu_2,\sigma_2^2)$

$Y=(1-\Delta)Y_1+\Delta Y_2$, $\Delta\in\{0,1\}$, $Pr(\Delta=1)=\pi$

$\phi_\theta(x)$ normal pdf with parameters $\theta=(\mu,\sigma^2)$

pdf of Y $g_Y(y)=(1-\pi)\phi_{\theta_1}(y)+\pi\phi_{\theta_2}(y)$

want to estimate parameter $\theta=(\pi,\theta_1,\theta_2)=(\pi,\mu_1\sigma_1^2,\mu_2\sigma_2^2)$ via MLE, $\ell(\theta|\bm Z)=\sum_{i=1}^N\log[(1-\pi)\phi_{\theta_1}(y_i)+\pi\phi_{\theta_2}(y_i)$

$\gamma_i(\theta)=E(\Delta_i|\theta,\bm Z)=Pr(\Delta_i=1|\theta,\bm Z)$

_Algorithm_

1. Take initial guesses for the parameters

2. Compute responsibilities

$\hat\gamma_i=\frac{\hat\pi\phi_{\theta_2}(y_i)}{(1-\hat\pi)\phi_{\theta_1}(y_i)+\hat\pi\phi_{\theta_2}(y_i)}$

3. Compute the weighted means and variances:

$\mu_1=\frac{\sum_{i=1}^N(1-\hat\gamma_i)y_i}{\sum_{i=1}^N(1-\hat\gamma_i)}$, $\hat\sigma_1=\frac{\sum_{i=1}^N(1-\hat\gamma_i)(y_i-\hat\mu_1)^2}{\sum_{i=1}^N(1-\hat\gamma_i)}$

$\mu_2=\frac{\sum_{i=1}^N\hat\gamma_iy_i}{\sum_{i=1}^N\hat\gamma_i}$, $\hat\sigma_2=\frac{\sum_{i=1}^N\hat\gamma_i(y_i-\hat\mu_2)^2}{\sum_{i=1}^N\hat\gamma_i}$

4. Iterate 2. and 3. until convergence.

General Expectation Maximization algorithm on page 277.

**MCMC**

Gibbs sampling ignores first $m$ samples to achieve stationarity

Can't have improper priors

**Bagging:** (282)

Can reduce MSE

bootstrap sample b $\bm Z^{*b}, b=1,...,B$

prediction $\hat f^{*b}(x)$ fit to bootstrap sample

Empirical distribution $\mathcal{\hat P}_n(t)$ for n observed data points is $\frac{\text{number of elements in the sample}\le t}{n}$ (binomial)

Theoretical bagging estimate? $E_\mathcal{\hat P}[\hat f^*(x)]$

Monte carlo estimate: $\hat f_{bag}(x)=\frac{1}{B}\sum_{b=1}^B\hat f^{*b}(x)$, asymptotically unbiased.

If original estimate $\hat f(x)$ is linear, bagged and original estimate are the same


**Bayesian Model Averaging** (288)

Set of candidate models $\mathcal{M_m},\ m=1,...,M$, training set $\bm{Z}$, $\zeta$ is a quantity of interest

$Pr(\zeta|\bm Z)=\sum_{m=1}^MPr(\zeta|\mathcal{M_m},\bm Z)Pr(\mathcal{M_m}|\bm Z)$ is a weighted average of predictions from each model with weights proportional to the posterior for each model.


**Stochastic search bumping** 290

Bumping uses bootstrap sampling to move randomly through model space. Draw bootstrap samples and fit model to each sample. Choose the model that best fits the original training set.

Idea is if a portion of the data causes a bad fit, a bootstrapped sample with out that portion of the data may result in a better estimated model. Models need relatively similar levels of complexity.

Are bootstrap samples size of full training set?

**CHAPTER 9**

**Additive Regression**

Form: $E[Y|X_1,X_2,...,X_p]=\alpha+f_1(X_1)+f_2(X_2)+...+f_p(X_p)$, where $f_i$ is a parametric, or smooth nonparametric function.

Semiparametric: $g(\mu)=\bm X^T\bm\beta+\alpha_k+f(Z)$ adds a parametric component and nonparametric component.

$\text{Ave}(\cdot)$ is the average function. Is there a difference between $\text{Ave}(\cdot)$ and $\text{ave}(\cdot)$?


$\mathcal{S}_j$ denotes smoothing operator $j$

**Tree-Based Methods**

Binary decision tree: partitions response based on binary decisions about features. For regression, each region has a constant assigned for the prediction. Uses sums of squares for loss.

$\hat c_m$ is the prediction and it is the average of the region of the response.

Tree size is a tuning parameter **hyper hyper parameter?**

Pruning: Using a large tree, remove non terminal nodes one by one to reduce complexity. The complexity cost criterion is given by finding the number of predictions that land in region $m$, multiplying that by the squared loss for these predictions, and summing over all regions of the tree. A penalty term proportional to the tree size is added to the complexity cost criterion. We choose $\alpha$ to minimize the complexity cost via cross validation. Recommended 5 or 10 fold.

More than 2 splits is generally not favorable as data becomes too fragmented.

Tree algorithms are highly variable because of hierarchical structure. Also results in a non-smooth prediction for regression.
















