---
title: 'DeVries Writing Project'


format:
  html: default
  pdf:
    toc: true
    number-sections: true
    cite-method: natbib
    include-in-header: 
      - "WPROJ.sty"
    include-before-body:
      - "coversheet.tex"

fontsize: 12pt
linestretch: 2
bibliography: references.bib

editor: source
---

```{r setup}
#| include: false
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(tidyverse)
library(plotly)
library(ggpubr)
library(randomForest)
library(fields)
library(mnormt)
library(DiagrammeR)
library(future)
library(future.apply)
```

\newpage

# Introduction {#sec:intro}

This is my introduction, Section \ref{sec:intro}. This problem is important because... Here is the motivation for my study.

In \ref{sec:background}, I will review the literature on this topic. \ref{sec:methods} describes my methods. Finally, in \ref{sec:conclusion}, I will discuss the implications of this research and future work.

## Subsection

This is a subsection in my Introduction section.

# Literature Review {#sec:background}

Statistical Modelling: the Two Cultures \citep{Breiman_2001} describes two approaches to data problems; data, and algorithmic models. Data modelling is described as classical statistics where data is assumed to be generated by some data generating stochastic process. In some cases, this is well warranted by previous research. In many cases though, the true underlying process contains complex dynamics that may or may not be observable. Algorithmic modelling sub goes distributional assumptions, instead focusing on finding the optimal function $f$ to map the feature set $\bm X$ to $\bm y$. Here, we provide a brief review of tree based methods, including random forests and Bayesian Additive Regression Trees.\\

Decision trees are a type of hierarchical model where the the feature space is partitioned into hyperrectangular regions, and a model is fit for each region. For regression tasks, we typically use a mean only model. Unlike our mean only models for linear regression, we do not make any distributional assumptions. Without a distribution, we can't quantify the uncertainty in our predictions or the decision boundaries. Additionally, the binary structure of trees makes the effects of individual predictors less interpretable than a linear model. A change in one of our predictors $x_{i,j}$ will not give a different prediction $\hat y_{i}$ unless the change is large enough to result in a different outcome to the binary decision. On the other hand, trees greatly outperform linear models with more complex data. The hierarchical structure naturally handles multicollinearity, and can help us find interactions between covariates. An example of a tree model is shown in (). Every decision after the top root node is conditioned on the previous decisions, allowing us to display the hierarchy graphically.

```{mermaid}
flowchart TD
  A[$X_1>a$]
  A --> |Yes| B[$X_2>b$]
  A --> |No| C[$X_3>c$]
  B --> |Yes| D($\mu_1$)
  B --> |No| E($\mu_2$)
  C --> |Yes| F[$X_4>d$]
  C --> |No| G($\mu_3$)
  F --> |Yes| H($\mu_4$)
  F --> |No| I($\mu_5$)
```

As mentioned previously, regression trees will use a mean only model. The tree is then a step function and for any training set $\bm X$ with target $\bm Y$ where $y_i|\bm X_i= y_j|\bm X_i$ for all $i,j$, we can fit a tree that will have no error on the training set.  With this flexibility, over fitting is a major concern. We can control the overall size and shape of trees with three hyper-parameters. First is the complexity parameter $\alpha$. $\alpha$ scales the penalty which is based on the number of terminal nodes (mean only models). Next we have the minimum split and minimum bucket sizes. The minimum bucket is the fewest number of observations allowed in a terminal node. Similarly, the minimum split is the least number of observations allowed in one outcome of a binary decision. We generally choose the complexity parameter by growing the largest possible tree, then removing terminal nodes with fewer observations. Next we compare the trees via cross validation and choose the tree with minimal cross validated error. This process of choosing an appropriate tree size is commonly referred to as pruning.
  
While pruning helps reduce the risk of over fitting, large regression trees are inherently unstable. We shall demonstrate this by comparing the number of splits determined by cross validation for data generated by the following model.

$$y_i=\cos(108t_i\sin(108t_i+x_i))\cdot\exp(\sin(108t_i\cos(108t_i+x_i)))+x_i$$
$$X_i\sim N(0,\sigma)$$
$$t_i=-\frac{\pi}{18},...,\frac{\pi}{18}$$
$$i=1,2,...,350$$

```{r cache=TRUE}
i <- seq(-pi / 18, pi / 18, 0.001)
noise <- 0

noises <- rep(c(0.1, 0.15, 0.2), each = 100) |> matrix()
sim_res <- apply(noises, 1, function(noise) {
  x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  train <- data.frame(
    x = x,
    y = y,
    z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise),
    i = i
  )
  x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  test <- data.frame(
    x = x,
    y = y,
    z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise),
    i = i
  )
  tree0 <- rpart(data = train, z ~ i, control =
                   rpart.control(minsplit = 1, minbucket = 1, cp = 0))
  opt_i <- which(tree0$cptable[, 4] == min(tree0$cptable[, 4])) |>
    as.vector()
  if (length(opt_i) > 1) {
    opt_i <- min(opt_i)
  }
  tree1 <- rpart(data = train, z ~ i, control =
                   rpart.control(minsplit = 1, minbucket = 1,
                                 cp = tree0$cptable[opt_i, 1]))
  
  return(c(tree0$cptable[opt_i, c(1, 2, 4)], noise,
           mean((test$z - predict(tree1, newdata = test)) ^ 2)))
}) |> t() |> as.data.frame()

sim_res <- sim_res |> dplyr::rename("Sigma" = "V4")


sim_theme <- theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank(),
        panel.background = element_blank(),
        strip.background.x = element_blank(),
        legend.position = "none")

ggplot(sim_res) +
  geom_violin(aes(x = Sigma, y = nsplit, fill = Sigma, group = Sigma)) +
  sim_theme +
  scale_fill_gradient(low = "darkgreen", high = "gold3") +
  labs(x = "Standard Deviation of Noise (Sigma)",
       y = "Optimal Number of Splits",
       title = "Optimal Number of Splits Chosen by Cross Validation")
```

Decision trees are very unstable for this data. Across all three noise levels, the number of splits chosen by cross validation ranges from zero to over 300. The variability in the size/structure of trees suggest we can't have much faith in predictions from an individual tree. This led to the development of the random forest algorithm by Leo Breiman and Adele Cutler. Random forests are an adaptation of bagging. Bagging is simply averaging the predictions of models fit to bootstrapped samples. This reduces the variability of predictions at the cost of bias. \todo{Provide formal explanation of bias created by bagging}. The random forest algorithm randomly selects subsets of all the available predictors and fits decision tree to bootstrapped samples. A fitted value $y_i|\bm x_i$ is obtained by averaging predictions from all trees where $\bm x_i$ was not in the bootstrap sample. This leads to more stable predictions as seen below.



```{r cache=TRUE}
sim_res_forest <- apply(noises, 1, function(noise) {
  x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  train <- data.frame(
    x = x,
    y = y,
    z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise),
    i = i
  )
  x1 <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y1 <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  test <- data.frame(
    x = x1,
    y = y1,
    z = cos(x1) * exp(sin(y1)) + rnorm(length(i), 0, noise),
    i = i
  )
  ran0 <- randomForest(data = train, z ~ i)
  return(c(mean((test$z - predict(ran0, newdata = test)) ^ 2), noise))
    
}) |> t() |> as.data.frame() |> dplyr::rename("Sigma" = "V2")

mse_both <- data.frame(MSE = c(sim_res_forest$V1, sim_res$V5),
                       Sigma = c(sim_res_forest$Sigma, sim_res$Sigma),
                       Model = rep(c("Random Forest", "Decision Tree"),
                                   each = 300))

ggplot(mse_both) +
  geom_violin(aes(x = Sigma, y = MSE, group = Sigma, fill = Sigma)) +
  facet_wrap(~ Model) +
  scale_fill_gradient(low = "darkgreen", high = "gold3") +
  sim_theme +
  labs(x = "Standard Deviation of Noise and Model Type",
       y = "MSE on New Dataset")
```

We see considerably less variation in the mean squared error of random forests fit to the same data as individual decision trees, but single tree models generally outperformed a random forest. Random forests are designed for data with several covariates. With a single predictor, the random forest algorithm degenerates to bagging with out of bag predictions. Bagging reduces the variance of predictions at the cost of bias, and thus the "random forest" predictions were often worse than predictions from individual trees. In cases where we have access to several predictors, large trees will create complex hierarchies that are unlikely to exist in the true model. The following simulation study shows the random forest algorithm consistently outperform a single tree for data generated under a linear model with correlated covariates.

```{r cache=TRUE}
plan(multisession)
coords <- expand.grid(seq(0, 9), seq(0, 9))
dists <- dist(coords, diag = T, upper = T) |> 
  as.matrix(nrow = 10, ncol = 10)
sig <- Matern(dists, range = 1, phi = 10, smoothness = 2)
beta <- apply(as.matrix(1 : 50), 1, function(a) {return(c(rnorm(1, 1, 5),
                                                          rnorm(1, -1, 5)))}) |> 
  as.vector()

sim2_res <- future_apply(matrix(1 : 100), 1, function (a) {
  X_T1 <- rmnorm(500, mean = seq(3, 10, length.out = 100), varcov = sig)
  X_V1 <- rmnorm(500, mean = seq(3, 10, length.out = 100), varcov = sig)
  X_T2 <- X_T1[, 1 : 50]
  X_V2 <- X_V1[, 1 : 50]
  X_T3 <- X_T1[, 1 : 25]
  X_V3 <- X_V1[, 1 : 25]
  X_T4 <- X_T1[, 1 : 4]
  X_V4 <- X_V1[, 1 : 4]
  Y_T1 <- X_T1 %*% beta + rnorm(500, 0, 1)
  Y_V1 <- X_V1 %*% beta + rnorm(500, 0, 1)
  Y_T2 <- X_T2 %*% beta[1 : 50] + rnorm(500, 0, 1)
  Y_V2 <- X_V2 %*% beta[1 : 50] + rnorm(500, 0, 1)
  Y_T3 <- X_T3 %*% beta[1 : 25] + rnorm(500, 0, 1)
  Y_V3 <- X_V3 %*% beta[1 : 25] + rnorm(500, 0, 1)
  Y_T4 <- X_T4 %*% beta[1 : 4] + rnorm(500, 0, 1)
  Y_V4 <- X_V4 %*% beta[1 : 4] + rnorm(500, 0, 1)
  X_V1 <- X_V1 |> as.data.frame()
  X_V2 <- X_V2 |> as.data.frame()
  X_V3 <- X_V3 |> as.data.frame()
  X_V4 <- X_V4 |> as.data.frame()

  tree0_100 <- rpart(formula = Y_T1 ~ X_T1, control =
                 rpart.control(cp = 0, minsplit = 1, minbucket = 1))
  opt100 <- which(tree0_100$cptable[, 4] == min(tree0_100$cptable[, 4])) |>
    as.vector()
  if (length(opt100 > 1)) {
    opt100 <- min(opt100)
  }
  tree1_100 <- rpart(formula = Y_T1 ~ X_T1, control =
                 rpart.control(cp = tree0_100$cptable[opt100, 1],
                               minsplit = 1, minbucket = 1))
  tree0_50 <- rpart(formula = Y_T2 ~ X_T2, control =
                 rpart.control(cp = 0, minsplit = 1, minbucket = 1))
  opt50 <- which(tree0_50$cptable[, 4] == min(tree0_100$cptable[, 4])) |>
    as.vector()
  if (length(opt50) != 1) {
    opt50 <- opt50[which(tree0_50$cptable[opt50, 2] ==
                           min(tree0_50$cptable[opt50, 2]))]
  }
  tree1_50 <- rpart(formula = Y_T2 ~ X_T2, control =
                 rpart.control(cp = tree0_50$cptable[opt50, 1],
                               minsplit = 1, minbucket = 1))
  tree0_25 <- rpart(formula = Y_T3 ~ X_T3, control =
                 rpart.control(cp = 0, minsplit = 1, minbucket = 1))
  opt25 <- which(tree0_25$cptable[, 4] == min(tree0_25$cptable[, 4])) |>
    as.vector()
  if (length(opt25) != 1) {
    opt25 <- opt25[which(tree0_25$cptable[opt25, 2] == 
                           min(tree0_25$cptable[opt25, 2]))]
  }
  tree1_25 <- rpart(formula = Y_T3 ~ X_T3, control =
                 rpart.control(cp = tree0_25$cptable[opt25, 1],
                               minsplit = 1, minbucket = 1))
  tree0_4 <- rpart(formula = Y_T3 ~ X_T3, control =
                 rpart.control(cp = 0, minsplit = 1, minbucket = 1))
  opt4 <- which(tree0_4$cptable[, 4] == min(tree0_4$cptable[, 4])) |> as.vector()
  if (length(opt4) != 1) {
    opt4 <- opt4[which(tree0_4$cptable[opt4, 2] ==
                           min(tree0_4$cptable[opt4, 2]))]
  }
  tree1_4 <- rpart(formula = Y_T4 ~ X_T4, control =
                 rpart.control(cp = tree0_4$cptable[opt4, 1],
                               minsplit = 1, minbucket = 1))
  
  
  ran_100 <- randomForest(x = X_T1, y = Y_T1, ntree = 800, importance = T)
  ran_50 <- randomForest(x = X_T2, y = Y_T2, ntree = 800, importance = T)
  ran_25 <- randomForest(x = X_T3, y = Y_T3, ntree = 800, importance = T)
  ran_4 <- randomForest(x = X_T4, y = Y_T4, ntree = 800, importance = T)
  
  return(c(mean((Y_V4 - predict(tree1_4, newdata = X_V4)) ^ 2),
           mean((Y_V3 - predict(tree1_25, newdata = X_V3)) ^ 2),
           mean((Y_V2 - predict(tree1_50, newdata = X_V2)) ^ 2),
           mean((Y_V1 - predict(tree1_100, newdata = X_V1)) ^ 2),
           mean((Y_V4 - predict(ran_4, newdata = X_V4)) ^ 2),
           mean((Y_V3 - predict(ran_25, newdata = X_V3)) ^ 2),
           mean((Y_V2 - predict(ran_50, newdata = X_V2)) ^ 2),
           mean((Y_V1 - predict(ran_100, newdata = X_V1)) ^ 2)))
}, future.seed = 252) |> t()
sim_dat <- data.frame(y = c(sim2_res), 
                 x = rep(c(paste("Single Tree on", c(4, 25, 50, 100),
                                 "Predictors"),
                           paste("Random Forest on", c(4, 25, 50, 100),
                                 "Predictors")),
                         each = 100)) |> as.data.frame()
ggplot(data = sim_dat) +
  geom_jitter(aes(y = y, x = x, , color = x)) +
  geom_violin(aes(y = y, x = x)) +
  sim_theme +
  theme(axis.text.x.bottom = element_text(), legend.position = "none") +
  coord_flip() +
  labs(y = "MSE", x = "",
       title = "MSE for Random Forest and Individual Tree Models")
plan(sequential)
```

```{r include=FALSE}
x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)

true0 <- data.frame(
  x = x,
  y = y,
  z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise)
)

noise <- 0.5

x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)

true1 <- data.frame(
  x = x,
  y = y,
  z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise)
)

ran0 <- randomForest(data = true1, z ~ i, ntree = 250)
tree0 <- rpart(data = true1, z ~ i, control = rpart.control(minsplit = 1, minbucket = 1, cp = 0))
opt_i <- which(tree0$cptable[, 4] == min(tree0$cptable[, 4])) |> as.vector()
if (length(opt_i) != 1) {
  opt_i <- opt_i[which(tree0$cptable[opt_i, 2] == min(tree0$cptable[opt_i, 2]))]
}
tree1 <- rpart(data = true1, z ~ i,
               control = rpart.control(minsplit = 1, minbucket = 1, cp = tree0$cptable[opt_i, 1]))
func_dat <- data.frame(y = true1$z, i = i, Forest = ran0$predicted,
                       Tree = predict(tree1))
quant_theme <- theme(panel.background = element_rect(fill = "white"),
        panel.grid = element_line(colour = "grey"))

ggplot(func_dat) +
  geom_line(aes(x = i, y = Forest, color = factor("Forest"))) +
  geom_line(aes(x = i, y = Tree, color = factor("Tree"))) +
  geom_point(aes(x = i, y = y, color = factor("y")), alpha = 0.2) + 
  scale_color_manual(
    values = c("Forest" = "orange", "Tree" = "red", "y" = "darkgreen"),
    name = "Variables"
  ) +
  quant_theme
```




# Methods {#sec:methods}



```{=tex}
\begin{figure}[h]
\centering
\includegraphics[width=10cm]{figures/correlation}
\caption{My favorite statistical cartoon \citep{xkcd}.}
\label{cartoon}
\end{figure}
```
Then, later in the text, we may reference Figure \ref{cartoon}.

# Conclusion {#sec:conclusion}

Amazing conclusions will be described in this section.
