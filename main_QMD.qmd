---
title: 'DeVries Writing Project'


format:
  pdf:
    toc: true
    number-sections: true
    cite-method: natbib
    include-in-header: 
      - "WPROJ.sty"
    include-before-body:
      - "coversheet.tex"

fontsize: 12pt
linestretch: 2
bibliography: references.bib

editor: source
---

```{r setup}
#| include: false
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(tidyverse)
library(plotly)
library(ggpubr)
library(randomForest)
library(fields)
library(mnormt)
```

\newpage

# Introduction {#sec:intro}

This is my introduction, Section \ref{sec:intro}. This problem is important because... Here is the motivation for my study.

In \ref{sec:background}, I will review the literature on this topic. \ref{sec:methods} describes my methods. Finally, in \ref{sec:conclusion}, I will discuss the implications of this research and future work.

## Subsection

This is a subsection in my Introduction section.

# Literature Review {#sec:background}

Statistical Modelling: the Two Cultures \citep{Breiman_2001} describes two approaches to data problems; data, and algorithmic models. Data modelling is described as classical statistics where data is assumed to be generated by some data generating stochastic process. In some cases, this is well warranted by previous research. In many cases though, the true underlying process contains complex dynamics that may or may not be observable. Algorithmic modelling sub goes distributional assumptions, instead focusing on finding the optimal function $f$ to map the feature set $\bm X$ to $\bm Y$. Here, we provide a brief review of tree based methods, including random forests. All notes are based on the Elements of Statistical Learning text \citep{Hastie_Tibshirani_Friedman_2009}.\\
Decision trees 

# Methods {#sec:methods}
#tcosθ−f(t)sinθ, y=tsinθ+f(t)cosθ
To investigate the effect of sample size and the number of terminal nodes on accuracy

```{r}
i <- seq(-pi / 18, pi / 18, 0.0001)
noise <- 1

x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)

true <- data.frame(
  x = x,
  y = y,
  z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise)
)
tree0 <- rpart(data = true, z ~ i, control =
  rpart.control(cp = 0, minsplit = 1, minbucket = 1))
plot_ly() %>%
  add_trace(
    data = true,
    x = ~ true$x, y = ~ true$y, z = ~ true$z,
    mode = 'markers',
    type = 'scatter3d',
    marker = list(color = 'salmon', size = 6, opacity = 0.5),
    name = 'True Values'
  ) %>%
  add_trace(
    x = true$x, y = ~ true$y, z = ~ predict(tree0),
    mode = 'markers',
    type = 'scatter3d',
    marker = list(color = 'black', size = 2, opacity = 1),
    name = 'Predictions'
  ) %>%
  layout(
    title = "3D Scatter Plot of System With no Noise",
    scene = list(
      xaxis = list(title = 'X'),
      yaxis = list(title = 'Y'),
      zaxis = list(title = 'Z')
    )
  )
ggplot() +
  geom_point(aes(x = i, y = true$z)) +
  labs(title = "Response vs Predictor", x = "Response", y = "Predictor")
ggplot() +
  geom_line(aes(x = true$z, y = predict(tree0)))
sum((predict(tree0) - true$z) ^ 2)
```

```{r}
noises <- rep(seq(0, 1, length.out = 30), each = 50) %>% matrix()
sim_res <- apply(noises, 1, function(noise) {
  x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  train <- data.frame(
    x = x,
    y = y,
    z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise),
    i = i
  )
  x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  test <- data.frame(
    x = x,
    y = y,
    z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise),
    i = i
  )
  tree0 <- rpart(data = train, z ~ i, control =
                   rpart.control(minsplit = 0, minbucket = 1, cp = 0))
  opt_i <- which(tree0$cptable[, 4] == min(tree0$cptable[, 4])) %>% as.vector()
  if (length(opt_i) != 1) {
    opt_i <- opt_i[which(tree0$cptable[opt_i, 2] == min(tree0$cptable[opt_i, 2]))]
  }
  tree1 <- rpart(data = train, z ~ i, control =
                   rpart.control(minsplit = 0, minbucket = 1,
                                 cp = tree0$cptable[opt_i, 1]))
  
  return(c(tree0$cptable[opt_i, c(1, 2, 4)], noise,
           mean((test$z - predict(tree1, newdata = test)) ^ 2)))
}) %>% t() %>% as.data.frame()


ggplot(sim_res) +
  geom_point(aes(x = V4, y = CP), alpha = 0.2, colour = "maroon") +
  labs(x = "Standard Deviation of Noise", y = "Optimal Complexity Parameter")

ggplot(sim_res) +
  geom_point(aes(x = V4, y = nsplit), alpha = 0.2, colour = "orange") +
  labs(x = "Standard Deviation of Noise", y = "Optimal Number of Splits")

ggplot(sim_res) +
  geom_point(aes(x = V4, y = xerror), alpha = 0.2, colour = "salmon") +
  labs(x = "Standard Deviation of Noise", y = "Standardized Cross Validation Error")

ggplot(sim_res) +
  geom_point(aes(x = V4, y = V5), alpha = 0.2, colour = "salmon") +
  labs(x = "Standard Deviation of Noise", y = "MSE on New Dataset")

ggplot() +
  geom_line(aes(x = i, y = true0$z), colour = "salmon") +
  geom_point(aes(x = i, y = predict(tree1)))
```

```{r}
sim_res_forest <- apply(noises, 1, function(noise) {
  x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  train <- data.frame(
    x = x,
    y = y,
    z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise),
    i = i
  )
  x1 <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y1 <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  test <- data.frame(
    x = x1,
    y = y1,
    z = cos(x1) * exp(sin(y1)) + rnorm(length(i), 0, noise),
    i = i
  )
  ran0 <- randomForest(data = train, z ~ i, ntree = 250)
  return(c(mean((test$z - predict(ran0, newdata = test)) ^ 2), noise))
    
}) %>% t() %>% as.data.frame()

noise <- 0

x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)

true0 <- data.frame(
  x = x,
  y = y,
  z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise)
)

noise <- 0.5

x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)

true1 <- data.frame(
  x = x,
  y = y,
  z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise)
)

ran0 <- randomForest(data = true1, z ~ i, ntree = 250)

ggplot() +
  geom_point(aes(x = i, y = true$z), alpha = 0.2)

ggplot() +
  geom_line(aes(x = i, y = true0$z), colour = "salmon") +
  geom_point(aes(x = i, y = ran0$predicted), alpha = 0.2)
```

Trees outperform random forests when we have a lot of data in comparison to the number of predictors.

```{r}
plan(multisession)
coords <- expand.grid(seq(0, 4), seq(0, 4))
dists <- dist(coords, diag = T, upper = T) %>% as.matrix(
  nrow = dim(coords) ^ 2, ncol = dim[coords] ^ 2)
sig <- Matern(dists, range = 1, phi = 10, smoothness = 2)

sim2_res <- future_apply(matrix(1 : 100), 1, function (a) {
  X_T <- rmnorm(50, mean = seq(-2, 0, length.out = 25), varcov = sig)
  X_V <- rmnorm(50, mean = seq(-2, 0, length.out = 25), varcov = sig)
  beta <- seq(0, 3, length.out = 25)
  Y_T <- X_T %*% beta + rnorm(50, 0, 1)
  Y_V <- X_V %*% beta + rnorm(50, 0, 1) + 200
  X_V <- X_V %>% as.data.frame()

  tree0 <- rpart(formula = Y_T ~ X_T, control =
                 rpart.control(cp = 0, minsplit = 1, minbucket = 1))
  opt_i <- which(tree0$cptable[, 4] == min(tree0$cptable[, 4])) %>% as.vector()
  if (length(opt_i) != 1) {
    opt_i <- opt_i[which(tree0$cptable[opt_i, 2] == min(tree0$cptable[opt_i, 2]))]
  }
  tree1 <- rpart(formula = Y_T ~ X_T, control =
                 rpart.control(cp = tree0$cptable[opt_i, 1],
                               minsplit = 1, minbucket = 1))
  ran0 <- randomForest(x = X_T, y = Y_T, ntree = 800, importance = T)
  return(c(mean((Y_V - predict(tree1, newdata = X_V)) ^ 2),
           mean((Y_V - predict(ran0, newdata = X_V)) ^ 2)))
}, future.seed = 252) %>% t()
sim_dat <- cbind(y = c(sim2_res), x = factor(rep(1 : 2, each = 100),
                                             labels = c("Tree", "Forest"))) %>%
  as.data.frame()
catstats2::enhanced_stripchart(data = sim_dat, y ~ x)
```




```{=tex}
\begin{figure}[h]
\centering
\includegraphics[width=10cm]{figures/correlation}
\caption{My favorite statistical cartoon \citep{xkcd}.}
\label{cartoon}
\end{figure}
```
Then, later in the text, we may reference Figure \ref{cartoon}.

# Conclusion {#sec:conclusion}

Amazing conclusions will be described in this section.
