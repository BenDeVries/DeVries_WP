---
latex-engine: xelatex
format:
  pdf:
    fig-pos: 'H'
    keep-tex: true
    cite-method: natbib
    include-in-header: 
      - "WPROJ.sty"
    include-before-body:
      - "coversheet.tex"
    toc: true
    embed-resources: true
    number-sections: true
  html:
    toc: true
    embed-resources: true
    number-sections: true
    cite-method: natbib
    include-in-header: 
      - "WPROJ.sty"
    include-before-body:
      - "coversheet.tex"
execute:
  fig-format: png
  dpi: 300
  warnning: false

fontsize: 12pt
linestretch: 2
bibliography: references.bib

editor: source
---

```{r setup}
#| include: false
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
setwd("~/DeVries_WP/")
library(rpart)
library(rpart.plot)
library(ggplot2)
library(tidyverse)
library(plotly)
library(ggpubr)
library(randomForest)
library(fields)
library(mnormt)
library(DiagrammeR)
library(future)
library(future.apply)
library(BART)
library(matrixStats)
library(ggnewscale)
library(raster)
library(sf)
library(geojsonsf)
library(scoringRules)
library(stringr)
library(viridis)
set.seed(406)
```

\newpage

# Introduction {#sec:intro}

Flying foxes are a fruit bat, with around 65 species native to parts of Asia, Australia, and Africa. In this project, we examine a population of grey headed, and black flying foxes surrounding Ballina, New South Wales Australia. Our interest in flying foxes stems from the Hendra virus, which is endemic to the four species of flying foxes in Australia. This rare virus is zoonotic, meaning it can be transferred from animals to humans. There is a fairly low chance of bats transmitting Hendra virus directly to people. Humans who have been infected with Hendra virus have come into contact with horses exposed to waste/bodily fluids from flying foxes.

Spreading a disease through an intermediate host of another species is referred to as a disease spillover. In *Pathogen Spillover Driven By Rapid Changes in Bat Ecology*, \cite{andy_art} identified loss of flying fox habitat and climate as influential factors in Hendra virus spillovers. Loss of habitat and climate both effect the availability of food and water, leading bats to venture into farmland and contaminate water/food supplies intended for horses. Understanding what factors influence flying fox feeding locations is thus naturally important to public health. For our analysis, we examine the feeding locations of the previously mentioned flying fox population, assessing whether any associations with land cover may be drawn.

Our data on flying fox feeding locations was accessed through a PostgreSQL database. Data was collected using a Robin radar, perched upon a hill within Ballina. The radar is able to track many objects at once, providing us with the flight path, velocity, classification, and other details. Traditionally, human observers have been used to record location information for bats. Manual counting is obviously labor intensive, and potentially less precise than radar; especially after considering both black and grey-headed flying foxes are nocturnal. It's easy to imagine potential challenges in counting clouds of bats at dark. Additionally, the Robin radar observes a larger region than manual observation. Instead of counts at discrete observation sites, the Robin radar provides points over a continuous space.

When exploring associations, background information is always important. \cite{queens_lc} examined the potential spatio-temporal relationship between changes in land cover and flying fox roost sites in Queensland, Australia. In Kolkata, India, \cite{indian_lc} analyzed types of trees and urban structures that attracted flying foxes with ANOVA. Both of these studies utilized highly detailed land cover data that involved manual survey. They also focused on differences in similar types of land cover, whereas our land cover classes are broader but less specific. Our end goal is not to perfectly describe the effects of land cover, but rather to see what types of land cover may be associated with flying fox feeding locations. Focusing on exploration, we turn to tree based models due to their inferential capabilities and modelling flexibility.

We begin with a brief review of supervised learning before discussing Classification and Regression Trees (CART) \citep{cart} along with several models inspired by CART. Next, our attention shifts to tools for obtaining and preparing data for our analysis. In this project we use Google Earth Engine to obtain our covariates, and access the response from an archived PostgreSQL database. Finally, we apply Bayesian Additive Regression Trees (BART) \citep{bart_paper} to analyze the feeding locations of flying foxes.

# Background {#sec:background}

## Supervised Learning

In traditional regression models we assume a specific relationship between the response, variables of interest, and controlling factors. Exploratory data analysis, background information, and model experimentation can bring light to potential relationships, but many problems involve unobservable dynamics that are difficult or impossible to capture via a parametric model. In some cases it is often necessary to take a more algorithmic approach to modelling. Unfortunately, many models in the algorithmic framework are referred to as black boxes, indicating the inner workings of a fitted model are difficult or impossible to explain in a meaningful way. 

Whether we choose to focus on predictive or explanatory modelling comes down to our end goal, and often there is some degree of balancing the two. For example, let's think about a model to classify whether someone has a disease. Obviously we care about the predictive capabilities. If the model is less accurate than an informed guess, it's not particularly useful. Focusing solely on prediction and ignoring explainability, we might implement a support vector machine. The support vector machine model would project patients measurements into a higher dimensional space and construct a hyper plane that separates the features into two groups, diseased/not diseased. Now if a doctor were to tell his patient, "We projected your measurements into a high dimensional space and unfortunately you fell on the wrong side of the hyperplane", the patient is almost certainly walking out.

Our doctor can soften the delivery; for example, they might adjust a patient's data and use the trained support vector machine to see how changes affect the predicted outcome. But patients are likely to ask how these changes impact their disease status. Logistic regression is a great example of an explainable model. Using the logistic regression model, the doctor might tell his patient "Looking at individuals similar to yourself, we've found an extra 30 minutes of exercise is associated with decreasing the odds of disease by a factor of X." Our doctor still has to interpret a mathematical formula and odds may be somewhat abstract to the patient, but there is a concise explanation.

Decision tree ensembles sit within the black box framework, but the explainability of individual trees has lead to establishing variable importance and interaction measures along with other useful tools for inference. It is still very difficult to interpret the effects of individual predictors in these models, as the hierarchical structure can make marginal effects very misleading. Still, variable importance measures combined with predictive accuracy make ensembles of trees a great tool for exploratory analysis.

Working with ensembles of trees, the models are not explainable in a meaningful way as a individual prediction may involve hundreds or thousands of decisions. We forego explaining an ensemble's reasoning and instead ask two questions: How accurate is the model? And which variables were useful in the model? If our model can make good predictions, then variables deemed important have some sort of association with our response, even if it's indirect.

Prediction based modelling has led to the development of a variety of algorithms that focus on fitting the data as opposed to explaining it. These algorithms can be much more complex than linear models. *The Elements of Statistical Learning* \citep{esl} was a key source for understanding algorithms and concepts in statistical learning. This text demonstrates situation where classic statistical models fall short, showing how complex models can solve intricate problems. Algorithms are explained in rigorous detail, while still providing intuition to implementations and capabilities.

Our background discussion begins with some notation and concepts in supervised learning. We then discuss the algorithms for several models, using simulation to demonstrate their use. In supervised learning problems, the goal is to map a set of features to a target. When discussing these components, we'll follow the notation used in \citep{esl}. $X$ denotes a features while $Y$ and $G$ denote the target for quantitative and categorical responses respectively. If there are $N$ observations of $p$ inputs, then $\bm X$ is an $N\times p$ matrix. The features for the $i$th observation are $\bm x_i$ while $\bm x_j$ denotes the vector of all observations for the $j$'th feature.

A common approach in supervised learning is to assume our features and target are observation of random variables with joint distribution $\text{Pr}(Y,X)$. $G$ would replace $Y$ in a classification problem and $\bm X$ would replace $X$ for multiple covariates. Letting $\mathcal{X,\ Y}$ denote the supports of $X, Y$, we want to find a function $f:\mathcal{X}\rightarrow\mathcal{Y}$ that predicts $Y$ reasonably well. To do this, we define a loss function $L(Y,f(X))$ which penalizes the prediction error on a training set $\boldsymbol \tau=(\bm x_1,y_1),...,(\bm x_N,y_N)$.

Our choice of loss function determines the optimal solution to the problem. The expected prediction error (EPE) is defined as $\mathbb{E}[L(Y,f(X))]$. Our overall goal is to approximate the function $f$ that minimizes the EPE. Using squared errors for loss $(L_2)$, the EPE is minimized by choosing $f$ such that $f(x)=\mathbb{E}[Y|X=x]$ (The Elements of Statistical Learning, pg. 18 \citep{esl}).

Now we need an algorithm to approximate $f$ using $\boldsymbol \tau$. Choosing an appropriate algorithm will be heavily influenced by the availability and distribution of data. With a quantitative target, we might be able to assume an additive error relationship between $Y$ and $\bm X$.

The additive error model is simply $Y=f(\bm X)+\epsilon$ where $\mathbb{E}[\epsilon]=0$ and $\bm X\perp\epsilon$. Here we are assuming the process that generated $Y$ can be approximated using $\bm X$ such that the error due to ignored features and other factors has mean zero. For data that was generated under an additive error model, a natural approach to approximate $f$ is to use a linear combination of basis functions. A set of functions $\mathcal{B}$ form a basis if for every function $f:\mathcal{X}\rightarrow\mathcal{Y}$, there exists a unique linear combination of elements of $\mathcal{B}$ that is equal to $f$. The additive model is defined as $Y=\beta_0+\sum_{j=1}^pf_j(\bm x_j)+\epsilon$.

One limitation of an additive model is we assume no interactions between features. There's no reason we can't add interactions or functions of multiple covariates, but identifying these interactions is difficult. We might be able to fit a model with select transformations of covariates and every possible interaction, but this would almost certainly result in overfitting. Assuming a generating model with any form of noise implies even the optimal $f$ will not predict every observation perfectly. A model is overfit when trends are ignored to explain slight irregularities in a sample. Discerning irregularities is highly subjective. Any decision we make based on observed data may lead to overfitting. To compare models predictive capability, we use data that has been held out from training to obtain a sample of prediction errors. With this sample, we can use some metric (usually based on our loss function) for model validation.

There are a variety of methods to perform model validation, but the two most common are hold out and K-fold cross validation. In hold out, some portion of the data is held out for testing and the rest is used to fit the model. If the model contains hyper parameters, we could split the data into training, validation, and testing. Fitting models to the training set, we can try various values for hyper parameters and compare model performance for predictions on the validation set. After we determine the optimal hyper parameter values, we refit the model on the combined training and validation set before using the testing set to assess the final fit. In some cases, additional tuning may be required after combining training and validation as additional training data may change the optimal values of hyperparameters. Using training/testing splits is generally favored for computationally intensive problems with observations to spare. The primary disadvantage to hold out validation is we can't use as much data for training.

In K-fold cross validation, we get to train and test using all available data by first splitting the data into $k$ folds of size $\approx N/k$. One fold is held for testing and $k-1$ are used for training. This is then repeated until all folds have been tested. A metric can be then used to select the model before fitting it to the complete dataset. K-fold makes the most of the available data, but is more computationally expensive. Additionally, K-fold does not assess the final model fit as parameter estimates will change as the model is refit. For models with low variance, representativeness of the available data is really the only concern. If there is a lot of variation in estimated parameters across folds, our cross validation becomes more of an assessment of the theoretical as opposed to fit model. This can still useful for model selection and refinement.

## Decision Trees

Decision trees are a type of hierarchical model where the the feature space is partitioned into hyperrectangular regions, and a model is fit for each region. For regression tasks, we typically use mean only models. In this case the decision tree model is a step function. Unlike our mean only models for linear regression, we do not make any distributional assumptions. Without a distribution, we can't quantify the uncertainty in our predictions. Additionally, the binary structure of trees changes the interpretation of effects. Small changes in a continuous feature $x_{i,j}$ may not effect the associated prediction $\hat y_i$. The mathematical relationship defining a linear model is highly informative, but rather rigid. Trees greatly outperform linear models with more complex data. A hierarchical structure naturally handles multicollinearity, and can help us find interactions between covariates. Even though we don't believe $\bm X$ is related to $\bm y$ by a step function, we can gain insight to the true relationship by a step function approximation. An example of a tree model is shown in @fig-tree-examp. Every decision after the top root node is conditioned on the previous decisions, allowing us to display the hierarchy graphically.

![Visualization of binary decision tree](figures/mermaid-figure-1.png){#fig-tree-examp}

```{mermaid}
%%| include: false
%%| eval: false

flowchart TD
  A["$$X_1\ge a$$"]
  A --> |Yes| B["$$X_2\ge b$$"]
  A --> |No| C["$$X_3\ge c$$"]
  B --> |Yes| D("$$\mu_1$$")
  B --> |No| E("$$\mu_2$$")
  C --> |Yes| F["$$X_2\ge d$$"]
  C --> |No| G("$$\mu_3$$")
  F --> |Yes| H("$$\mu_4$$")
  F --> |No| I("$$\mu_5$$")
```

Step functions are very flexible. If there exists a function that can map the features of our training data to the observed response, then we can represent said function with a decision tree and achieve zero error over $\bm \tau$. With this flexibility, over fitting is a major concern. We can control the overall size and shape of trees with three hyper-parameters. First is the complexity parameter $\alpha$ that scales the penalty which is based on the number of terminal nodes (mean only models). Next we have the minimum split and minimum bucket sizes. The minimum bucket is the fewest number of observations allowed in a terminal node. Similarly, the minimum split is the least number of observations allowed in one outcome of a binary decision. We generally choose the complexity parameter by growing the largest possible tree, then removing terminal nodes with fewer observations. Next we compare the trees via cross validation and choose the tree with minimal cross validated error. This process of choosing an appropriate tree size is commonly referred to as pruning. While pruning helps reduce the risk of over fitting, large regression trees are inherently unstable.

When we say trees are unstable, we are essentially saying that a small change to the data can result in a wildly different fit. We demonstrate the variability in the structure of trees with a simulation study. Data for the simulation study is generated under the model defined by equations 1-4. 
$$y_i=\cos(108t_i\sin(108t_i+w_i))\cdot\exp(\sin(108t_i\cos(108t_i+x_i)))+z_i$$ {#eq-a}
$$W_i,X_i,Z_i\sim N(0,\sigma^2)$$ {#eq-b}
$$t_i=-\frac{\pi}{18},...,\frac{\pi}{18}$$ {#eq-c}
$$i=1,2,...,350$$ {#eq-d}
The rpart function developed by \cite{rpart} is used to fit our CART model. A simulated testing data set displaying the true function along with the fit from a tree chosen by cross validation on training is displayed in @fig-sim1-pred-vis. @fig-sim1-tree displays the number of splits for the optimal complexity parameter chosen by cross validation.

```{r}
#| label: fig-sim1-pred-vis
#| fig-cap: Visualization of simulated data and predictions show a decision tree is simply a step function.
#| echo: FALSE

set.seed(123)
i <- seq(-pi / 18, pi / 18, 0.001)
noise <- 0.3
x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)

true0 <- data.frame(
  x = x,
  y = y,
  z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise)
)

x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
xnn <- 108 * i * sin(i * 108)
ynn <- 108 * i * cos(i * 108)
znn <- cos(xnn) * exp(sin(ynn))

true1 <- data.frame(
  x = x,
  y = y,
  z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise)
)

ran0 <- randomForest(data = true0, z ~ i, ntree = 250)
tree0 <- rpart(data = true0, z ~ i,
               control = rpart.control(minsplit = 1, minbucket = 1, cp = 0))
opt_i <- which(tree0$cptable[, 4] == min(tree0$cptable[, 4])) |> as.vector()
if (length(opt_i) != 1) {
  opt_i <- opt_i[which(tree0$cptable[opt_i, 2] == min(tree0$cptable[opt_i, 2]))]
}
tree1 <- rpart(data = true0, z ~ i,
               control = rpart.control(minsplit = 1, minbucket = 1,
                                       cp = tree0$cptable[opt_i, 1]))
func_dat <- data.frame(y = true0$z, i = i, Forest = predict(ran0, newdata = true1),
                       Tree = predict(tree1, newdata = true1))
quant_theme <- theme(panel.background = element_rect(fill = "white"),
        panel.grid = element_line(colour = "grey"),
        legend.position = c(0.9, 0.1),
        legend.background = element_blank())

ggplot(func_dat) +
  geom_line(aes(x = i, y = znn, color = "True Function")) +
  geom_line(aes(x = i, y = Tree, color = factor("Tree"))) +
  geom_point(aes(x = i, y = z), alpha = 0.2, color = "darkgreen",
             data = true0) + 
  scale_color_manual(
    values = c("Tree" = "orange", "True Function" = "darkgreen"),
    name = ""
  ) +
  quant_theme +
  labs(y = "y", x = "t", title = "CART Fit on Simulated Dataset")

# ggplot(func_dat) +
#   geom_line(aes(x = i, y = znn, color = "True Function")) +
#   geom_line(aes(x = i, y = Forest, color = factor("Forest"))) +
#   geom_line(aes(x = i, y = Tree, color = factor("Tree"))) +
#   geom_point(aes(x = i, y = z), alpha = 0.2, color = "darkgreen",
#              data = true0) + 
#   scale_color_manual(
#     values = c("Forest" = "orange", "Tree" = "red", "True Function" = "darkgreen"),
#     name = ""
#   ) +
#   quant_theme +
#   labs(y = "y", x = "t",
#        main = "Random Forest")
```

```{r}
#| label: fig-sim1-tree
#| fig-cap: CART models can give similiar quality predictions with very different fits. Large variation in the structure of the model indicates inference can't be trusted with ugly data.
#| echo: FALSE

noises <- rep(c(0.1, 0.15, 0.2), each = 100) |> matrix()
sim_res <- apply(noises, 1, function(noise) {
  x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  train <- data.frame(
    x = x,
    y = y,
    z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise),
    i = i
  )
  x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  test <- data.frame(
    x = x,
    y = y,
    z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise),
    i = i
  )
  tree0 <- rpart(data = train, z ~ i, control =
                   rpart.control(minsplit = 1, minbucket = 1, cp = 0))
  opt_i <- which(tree0$cptable[, 4] == min(tree0$cptable[, 4])) |>
    as.vector()
  if (length(opt_i) > 1) {
    opt_i <- min(opt_i)
  }
  tree1 <- rpart(data = train, z ~ i, control =
                   rpart.control(minsplit = 1, minbucket = 1,
                                 cp = tree0$cptable[opt_i, 1]))
  
  return(c(tree0$cptable[opt_i, c(1, 2, 4)], noise,
           mean((test$z - predict(tree1, newdata = test)) ^ 2)))
}) |> t() |> as.data.frame()

sim_res <- sim_res |> dplyr::rename("Sigma" = "V4")


sim_theme <- theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank(),
        panel.background = element_blank(),
        strip.background.x = element_blank(),
        legend.position = "none")

ggplot(sim_res) +
  geom_violin(aes(x = factor(Sigma), y = nsplit, fill = Sigma, group = Sigma)) +
  sim_theme +
  scale_fill_gradient(low = "darkgreen", high = "gold3") +
  labs(x = "Standard Deviation of Noise (Sigma)",
       y = "Optimal Number of Splits",
       title = "Optimal Number of Splits Chosen by Cross Validation")
```

Decision trees are very unstable for this data. Across all three noise levels, the number of splits chosen by cross validation ranges from zero to over 300. The variability in the size/structure of trees suggest we can't have much faith in predictions from an individual tree. Slight changes to the data could result in a completely different tree structure, meaning our model may generalize poorly. A simpler model will be less variable, but more biased. To maintain low bias and reduce variance, we turn our attention to ensemble methods. Ensemble methods are quite prevalent in predictive modelling. By combining the predictions from several models, we can reduce the variance of predictions while capturing complex intricacies of the data.

The random forests algorithm, developed by \cite{random_forest}, is a CART-based ensemble combining Bootstrap Aggregating (Bagging) \citep{bagging} and Hoâ€™s random decision forests \citep{Ho}. Bagging is simply averaging the predictions of models fit to bootstrapped samples. When possible, we obtain $\hat y_i$ from averaging trees where $\bm x_i$ did not appear in training. These predictions are referred to as "out of bag predictions". Cross validation on out of bag predictions is not required as they are not based on the observed values of $\bm x_i$.

There's some intuition to the random forest algorithm. We know trees are flexible, and bagging will allow models to consider variations in the data. *The Elements of Statistical Learning* present several proofs demonstrating results that provide some intuition to the power of bootstrapping. On pages 271-272, \cite{esl} show that the distribution of bootstrap means for infinite samples from a normal population is equivalent to to the Bayesian posterior distribution with a non-informative prior. They also demonstrate that for the multinomial likelihood, the bootstrap distribution with infinite draws from the population approximates the Bayesian posterior assuming all outcomes are equi probable apriori. Later on pages 282-288, \cite{esl} show that the MSE of bagged estimates where samples are drawn from the population is less than or equal to the MSE of an individual model. For classification problems, \cite{bagging} shows that with infinite samples from the population, bagging can approximate the optimal classifier under certain conditions. This requires the majority of classifiers fit to bootstrapped samples to predict the correct class more often than not and relatively balanced classes. Otherwise, bagging can reduce accuracy. All of these results concerning infinite samples are not very useful, but hint at the practical applications of bagging.

The main limitation in bagging alone is correlation between models. When averaging results from two trees $T_1,T_2$, the variance of our estimates will be $\frac{1}{4}(\mathbb{V}[T_1]+\mathbb{V}[T_2]+2\cdot\text{cov}(T_1, T_2))$. If only a few variables contain relevant information, we expect trees to make similar predictions. The random forests algorithm of \cite{random_forest} combats the correlation between trees by taking the same approach implemented by Ho in random decision forests, \citep{Ho}. Random decision forests average multiple decision trees fit to the same data. The key feature in the algorithm is the random subspace method where a random subset of the available predictors is drawn for each decision. Introducing randomness brings variety to the forest, resulting in weaker correlation between trees. The combination of random subspace and bagging makes Breiman's random forest a flexible model with low variance. The hyperparameters for the random forest model are the number of trees to grow, the minimum and maximum number of observations allowed in a terminal node, and the number of predictors to select for each decision. In @fig-sim1-mse we compare the MSE of models fit to data simulated in the same way as the previous simulation study. The rpart \citep{rpart} and randomForest \citep{rf_r} functions were used to fit models.

```{r}
#| label: fig-sim1-rf-fit
#| fig-cap: Combining many step functions defined by decision trees leads to smooth predictions with impressive accuracy and precision.
#| echo: FALSE

ggplot(func_dat) +
  geom_line(aes(x = i, y = znn, color = "True Function")) +
  geom_line(aes(x = i, y = Forest, color = factor("Forest"))) +
  geom_point(aes(x = i, y = z), alpha = 0.2, color = "darkgreen",
             data = true0) + 
  scale_color_manual(
    values = c("Forest" = "orange", "True Function" = "darkgreen"),
    name = ""
  ) +
  quant_theme +
  labs(y = "y", x = "t", title = "Random Forest")
```

```{r}
#| label: fig-sim1-mse
#| fig-cap: Even if individual trees give better predictions than random forests on this data set the majority of the time, using single trees for predictions is risky because of the large variance.
#| echo: FALSE

sim_res_forest <- apply(noises, 1, function(noise) {
  x <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  train <- data.frame(
    x = x,
    y = y,
    z = cos(x) * exp(sin(y)) + rnorm(length(i), 0, noise),
    i = i
  )
  x1 <- 108 * i * sin(i * 108) + rnorm(length(i), 0, noise)
  y1 <- 108 * i * cos(i * 108) + rnorm(length(i), 0, noise)
  test <- data.frame(
    x = x1,
    y = y1,
    z = cos(x1) * exp(sin(y1)) + rnorm(length(i), 0, noise),
    i = i
  )
  ran0 <- randomForest(data = train, z ~ i)
  return(c(mean((test$z - predict(ran0, newdata = test)) ^ 2), noise))
    
}) |> t() |> as.data.frame() |> dplyr::rename("Sigma" = "V2")

mse_both <- data.frame(MSE = c(sim_res_forest$V1, sim_res$V5),
                       Sigma = c(sim_res_forest$Sigma, sim_res$Sigma),
                       Sigma1 = c(sim_res_forest$Sigma, sim_res$Sigma),
                       Model = rep(c("Random Forest", "Decision Tree"),
                                   each = 300))

ggplot() +
  geom_violin(aes(x = factor(Sigma), y = MSE, fill = Sigma, group = Sigma),
              data = mse_both |> filter(Model == "Decision Tree"), alpha = 0.5) +
  scale_fill_gradient(name = "Noise For\nDecision Tree", low = "lightgreen", high = "gold3") +
  new_scale_fill() +
  geom_violin(aes(x = factor(Sigma), y = MSE, fill = Sigma1, group = Sigma1),
              data = mse_both |> filter(Model == "Random Forest"), alpha = 0.5) +
  scale_fill_gradient(name = "Noise For\nRandom Forest", low = "darkgreen", high = "sienna") +
  sim_theme +
  theme(legend.position = "bottom",
        legend.text = element_blank()) +
  labs(x = "Standard Deviation of Noise and Model Type",
       y = "MSE on New Dataset",
       title = "MSE from Random Forest and Decision Tree Models on\nTesting Data")

```

There is considerably less variation in the mean squared error of random forests fit to the same data as individual decision trees, but single tree models generally outperformed a random forest. Random forests are designed for data with several covariates. With a single predictor, the random forest algorithm degenerates to bagging with out of bag predictions. Bagging reduces the variance of predictions at the cost of bias, and thus the random forest predictions were often worse than predictions from individual trees. Pages 600, 601 of The Elements of Statistical Learning, \citep{esl} display a proof showing that the bias of a random forest is the same as the bias of any individual tree in the ensemble. The randomization in the random forest algorithm implies trees within the ensemble are almost certainly smaller than a single tree fit to the data. In cases where we have access to several predictors, large trees will create complex hierarchies that are unlikely to exist in the true model. The following simulation study shows the random forest algorithm consistently outperform a single tree for data generated under a linear model with correlated covariates. Violin plots of MSE on testing data are displayed in @fig-sim2.

```{r}
#| label: fig-sim2
#| fig-cap: As relationships become more complex (i.e. more factors involved), mean squared error tends to increase. Generally, the increasing complexity has much less of an effect on random forests compared to individual trees.
#| echo: FALSE
#| warning: false

plan(multisession)
coords <- expand.grid(seq(0, 9), seq(0, 9))
dists <- dist(coords, diag = T, upper = T) |> 
  as.matrix(nrow = 10, ncol = 10)
sig <- Matern(dists, range = 1, phi = 10, smoothness = 2)
beta <- apply(as.matrix(1 : 50), 1, function(a) {return(c(rnorm(1, 1, 5),
                                                          rnorm(1, -1, 5)))}) |> 
  as.vector()

sim2_res <- future_apply(matrix(1 : 100), 1, function (a) {
  X_T1 <- rmnorm(500, mean = seq(3, 10, length.out = 100), varcov = sig)
  X_V1 <- rmnorm(500, mean = seq(3, 10, length.out = 100), varcov = sig)
  X_T2 <- X_T1[, 1 : 50]
  X_V2 <- X_V1[, 1 : 50]
  X_T3 <- X_T1[, 1 : 25]
  X_V3 <- X_V1[, 1 : 25]
  X_T4 <- X_T1[, 1 : 4]
  X_V4 <- X_V1[, 1 : 4]
  Y_T1 <- X_T1 %*% beta + rnorm(500, 0, 1)
  Y_V1 <- X_V1 %*% beta + rnorm(500, 0, 1)
  Y_T2 <- X_T2 %*% beta[1 : 50] + rnorm(500, 0, 1)
  Y_V2 <- X_V2 %*% beta[1 : 50] + rnorm(500, 0, 1)
  Y_T3 <- X_T3 %*% beta[1 : 25] + rnorm(500, 0, 1)
  Y_V3 <- X_V3 %*% beta[1 : 25] + rnorm(500, 0, 1)
  Y_T4 <- X_T4 %*% beta[1 : 4] + rnorm(500, 0, 1)
  Y_V4 <- X_V4 %*% beta[1 : 4] + rnorm(500, 0, 1)
  X_V1 <- X_V1 |> as.data.frame()
  X_V2 <- X_V2 |> as.data.frame()
  X_V3 <- X_V3 |> as.data.frame()
  X_V4 <- X_V4 |> as.data.frame()

  tree0_100 <- rpart(formula = Y_T1 ~ X_T1, control =
                 rpart.control(cp = 0, minsplit = 1, minbucket = 1))
  opt100 <- which(tree0_100$cptable[, 4] == min(tree0_100$cptable[, 4])) |>
    as.vector()
  if (length(opt100 > 1)) {
    opt100 <- min(opt100)
  }
  tree1_100 <- rpart(formula = Y_T1 ~ X_T1, control =
                 rpart.control(cp = tree0_100$cptable[opt100, 1],
                               minsplit = 1, minbucket = 1))
  tree0_50 <- rpart(formula = Y_T2 ~ X_T2, control =
                 rpart.control(cp = 0, minsplit = 1, minbucket = 1))
  opt50 <- which(tree0_50$cptable[, 4] == min(tree0_100$cptable[, 4])) |>
    as.vector()
  if (length(opt50) != 1) {
    opt50 <- opt50[which(tree0_50$cptable[opt50, 2] ==
                           min(tree0_50$cptable[opt50, 2]))]
  }
  tree1_50 <- rpart(formula = Y_T2 ~ X_T2, control =
                 rpart.control(cp = tree0_50$cptable[opt50, 1],
                               minsplit = 1, minbucket = 1))
  tree0_25 <- rpart(formula = Y_T3 ~ X_T3, control =
                 rpart.control(cp = 0, minsplit = 1, minbucket = 1))
  opt25 <- which(tree0_25$cptable[, 4] == min(tree0_25$cptable[, 4])) |>
    as.vector()
  if (length(opt25) != 1) {
    opt25 <- opt25[which(tree0_25$cptable[opt25, 2] == 
                           min(tree0_25$cptable[opt25, 2]))]
  }
  tree1_25 <- rpart(formula = Y_T3 ~ X_T3, control =
                 rpart.control(cp = tree0_25$cptable[opt25, 1],
                               minsplit = 1, minbucket = 1))
  tree0_4 <- rpart(formula = Y_T3 ~ X_T3, control =
                 rpart.control(cp = 0, minsplit = 1, minbucket = 1))
  opt4 <- which(tree0_4$cptable[, 4] == min(tree0_4$cptable[, 4])) |> as.vector()
  if (length(opt4) != 1) {
    opt4 <- opt4[which(tree0_4$cptable[opt4, 2] ==
                           min(tree0_4$cptable[opt4, 2]))]
  }
  tree1_4 <- rpart(formula = Y_T4 ~ X_T4, control =
                 rpart.control(cp = tree0_4$cptable[opt4, 1],
                               minsplit = 1, minbucket = 1))
  
  
  ran_100 <- randomForest(x = X_T1, y = Y_T1, ntree = 800, importance = T)
  ran_50 <- randomForest(x = X_T2, y = Y_T2, ntree = 800, importance = T)
  ran_25 <- randomForest(x = X_T3, y = Y_T3, ntree = 800, importance = T)
  ran_4 <- randomForest(x = X_T4, y = Y_T4, ntree = 800, importance = T)
  
  return(c(mean((Y_V4 - predict(tree1_4, newdata = X_V4)) ^ 2),
           mean((Y_V3 - predict(tree1_25, newdata = X_V3)) ^ 2),
           mean((Y_V2 - predict(tree1_50, newdata = X_V2)) ^ 2),
           mean((Y_V1 - predict(tree1_100, newdata = X_V1)) ^ 2),
           mean((Y_V4 - predict(ran_4, newdata = X_V4)) ^ 2),
           mean((Y_V3 - predict(ran_25, newdata = X_V3)) ^ 2),
           mean((Y_V2 - predict(ran_50, newdata = X_V2)) ^ 2),
           mean((Y_V1 - predict(ran_100, newdata = X_V1)) ^ 2)))
}, future.seed = 252) |> t()
sim_dat <- data.frame(y = c(sim2_res), 
                 x = rep(c(paste("Tree w/", c(4, 25, 50, 100),
                                 "Predictors"),
                           paste("RF w/", c(4, 25, 50, 100),
                                 "Predictors")),
                         each = 100)) |> as.data.frame()
ggplot(data = sim_dat) +
  geom_jitter(aes(y = y, x = x, , color = x), alpha = 0.2) +
  sim_theme +
  theme(axis.text.x.bottom = element_text(), legend.position = "none") +
  coord_flip() +
  labs(y = "MSE", x = "",
       title = "Data from Linear Model With Multicollinearity\nPresent")
plan(sequential)
```

## Bayesian CART

Bayesian tree algorithms assume a stochastic process generates the tree, and a distribution for the observations within the terminal nodes. Randomness is used to explore possible structures that may increase the posterior. The flexibility of trees necessitates regularization of their structure, which can be easily achieved through the use of a prior. There are many ways to specify a Bayesian tree but, our discussion is limited to the Bayesian CART method proposed by \cite{bayes_cart}. In Bayesian CART, the search process begins with an initial stump tree, and proposes a new structure for the tree at random. For the proposal, there are four possibilities. A change step is proposed with probability 0.4. In the change step, a non-terminal node is randomly selected and the splitting rule replaced. The new rule is chosen by randomly selecting a splitting variable $X_j$, and then randomly selecting an observed value of $\bm x_j$ as a cutoff. The same splitting rule cannot appear multiple times within the current tree. For the grow step of the search, a terminal node is randomly selected, a splitting rule is added, and a sibling for the chosen node is grown. The probability a grow step is 0.25 and the decision rule is chosen in the same way as in the change step. The third possible move in the search is randomly selecting intermediate nodes, and collapsing all nodes beneath. This step is referred to as pruning and is proposed with probability 0.25. The last possible move for a trees structure is node swapping. Parent and child intermediate nodes are selected and the rules are swapped. After updating the structure of the tree, new nodes are marked as splittable with probability $\frac{\alpha}{1+\beta\gamma^d}$ where $\alpha\in(0,1)$ and $\beta,\gamma\in(0,\infty)$ are hyperparameters and $d$ is the depth of the selected node. Finally, terminal nodes are updated and the proposal is accepted or rejected. \cite{bayes_cart} describe methods for independent and correlated terminal nodes. With the proposal tree constructed, the Metropolis-Hastings algorithm is used to update the sequence of trees.

Bayesian CART may become stuck around a local mode where the next tree in the sequence provides little to no improvement. To combat this, a sequence of trees is generated until the change in posterior probability between trees in the sequence stabilizes. The algorithm is then restarted to search for another local mode. Convergence is tracked by comparing the posterior probability distribution of a tree to it's predecessors, keeping track of which trees have the highest probability. The frequency of tree structures explored is ignored, making Bayesian CART a stochastic search as opposed to a Monte Carlo based method like in BART. Averaging trees that have converged to a local mode results in a model similar to random forests. Unlike random forests there is a distribution for the predictions.

## Bayesian Additive Regression Trees (BART)

The other learning algorithm we explored was BART, or Bayesian Additive Regression Trees. For a continuous predictor, BART assumes $y_i\sim N(\sum_{b=1}^m g(\bm x_i;T_b,M_b), \sigma^2)$ where $T_b$ is the $b$'th tree, $M_b$ are the associated terminal nodes, and $g(\bm x_i;T_b,M_b)$ is the function that assigns $\bm x_i$ to $\mu_{lb}$, $l\in\{1,2,...,|M_b|\}$. Unlike the random forest algorithm, the trees are not bagged and each tree has access to each feature. Another key difference is the final model is based on the sum of all trees as opposed to an average. BART models use the same stochastic process to define the tree structure as Bayesian CART, but differs in model specification. In Bayesian CART, trees that have high posterior probability are recorded, but the number of occurrences for each structure is ignored. This makes Bayesian CART a stochastic search as opposed to a full MCMC. In BART, draws from terminal nodes remain after the tree changes structure. We thus have a probability distribution over the structure of the trees. Placing a highly informative prior on the terminal status of nodes encourages smaller trees, resulting in faster convergence and less variable predictions.

The algorithm starts by growing $M$ trees with a single terminal node (stumps). Gibbs sampling is used to iteratively update each tree conditioned on all other trees. Conditional updates are simplified by expressing $T_b|T_1,T_2,...T_{b-1},T_{b+1},...T_m$ as $T_b|R_b$ where $R_b$ denotes the distribution of residuals from predictions excluding $T_b$. Summing trees grown conditionally on one another makes the contributions of individual nodes small. \cite{bart_paper} state "... we can imagine the algorithm as analogous to sculpting a complex figure by adding and subtracting small dabs of clay." Conditional updates along with a highly informative prior on the terminal status of nodes allow the model to explore intricate relationships in the data, without overfitting.

BART is an adaptation of the original Bayesian CART algorithm, \citep{bayes_cart}. With the smaller tree sizes, the pruning proposal is modified to only prune terminal nodes. Even with this modification, trees will still collapse to a stump throughout MCMC. Convergence is aided by the somewhat unrealistic assumption of independent priors for each tree and the standard deviation. The joint prior is given by $$p((T_1, M_1), (T_2, M_2),...,(T_m, M_m),\sigma^2)=p(\sigma^2)\prod_{b=1}^mp(M_b|T_b)p(T_b)$$ where $p(M_b|T_b)=\prod_{l=1}^{|M_b|}p(\mu_{lb}|T_b)$, and $p(\sigma^2)\sim\nu\lambda/\chi^2_\nu$ with hyperparameters $\nu,\lambda$. The hyper parameters are selected by first choosing a point estimate $\hat\sigma$. By default, this is the residual standard deviation for a multiple linear regression model. $\nu$ is then fixed ($\nu\in[3,10]$ recommended) and $\lambda$ solved for by imposing the constraint $\text{Pr}(\sigma<\hat\sigma)=q$, where $q$ is an additional hyper parameter. In the prior for a tree $p(T_b)$, the probability that a node at depth $d\in\mathbb{N}$ is non-terminal is given by $\alpha(1+d)^{-\beta}$, $\alpha\in(0,1),\ \beta\in[0,\infty)$. A discrete uniform prior imposes the initial belief that each feature equally likely to be selected for a nodes splitting rule. Similarly, a discrete uniform across the observed values of the selected predictor serves as the prior for the cut point in the binary decision. A $N(|M|\mu_\mu,|M|\sigma^2_\mu)$ prior is assumed for each $\mu_{lb}|T_b$ where $|M|$ is the total number of terminal nodes across all trees. The hyperparameters $\mu_\mu$ and $\sigma_\mu$ are chosen based on the data such that $\min(Y)=|M|\mu_\mu-k\sqrt{|M|}\sigma_\mu$ and $\max(Y)=|M|\mu_\mu-k\sqrt{|M|}\sigma_\mu$. In \citep{bart_paper}, they recommend choosing $k\in[1,3]$. The BART R package \citep{bart_r} rescales $Y$ such that $Y\in[-0.5, 0.5]$ and chooses $\mu_\mu=0\implies\sigma_\mu=\frac{0.5}{k\sqrt{|M|}}$.

Two differences between BART and traditional Bayesian models are the number of trees $m$ is fixed and data is used to inform the priors. These features push BART to be more of a non-parametric method, but with many of the features of Bayesian models. Inference is still derived from the posterior and hyperparameters can be tuned using a testing set with the goal of bringing prediction intervals to the nominal level. An example of a BART model tuned for nominal coverage to simulated data is displayed in @fig-bart-sim1. The poor fit can likely be explained by non-additive noise in the generating function. BART is incredibly, flexible but at the end of the day it is still an additive model.

```{r}
#| echo: FALSE
#| output: FALSE
#| include: FALSE
#| eval: false

# n <- 250
# X <- runif(n * 6, -5, 5) |> matrix(ncol = 6) |> data.frame()
# pairs <- list(c(1, 6), c(1, 2), c(1, 3), c(2, 6))
# y <- rowSums2(sapply(pairs, function(a) sqrt(abs((X[, a[1]] * X[, a[2]]))))) +
#   sin(X[, 1]) + cos(X[, 2]) + log(abs(X[, 3])) + X[, 4] ^ 2 +
#   floor(X[, 5]) + ceiling(X[, 6]) + rnorm(n)
# 
# m_bart <- wbart(x.train = X, y.train = y, 
#                 ntree = 49, ndpost = 1500, nskip = 1500)
# bartDiag(m_bart, response = y, data = X)
# 
# bart_dat <- extractTreeData(m_bart, data = X)
# vsupMat <- viviBartMatrix(bart_dat,
#                           type = 'vsup',
#                           metric = 'propMean',
#                           metricError = "CV")
# viviBartPlot(vsupMat,
#              max_desat = 1,
#              pow_desat = 0.6,
#              max_light = 0.6,
#              pow_light = 1,
#              label = 'Coeff. of Variation') +
#   ggtitle("Variable Importance and Interaction Measures Example")
# 
# plotTrees(bart_dat, iter = 1) +
#   ggtitle("Structure of Trees for First Iteration After Burn")
```

```{r}
#| echo: FALSE
#| include: FALSE
#| output: false

post0 <- wbart(x.train = true0$x, y.train = true0$z,
               x.test = true1$x, ntree = 250, cont = T, k = 0.5, lambda = 220,
               base = 0.99, power = 0.5, sigquant = 0.99, sigest = 2 * sd(true0$y))
pred <- predict(post0, newdata = matrix(true1$x))
pred_quants <- colQuantiles(pred, probs = c(0.025, 0.975))

mean(between(true1$z, pred_quants[, 1], pred_quants[, 2]))

mean((true1$z - pred_quants[, 2]) ^ 2)
mean((true1$z - predict(ran0, newdata = true1$z)) ^ 2)
```

```{r}
#| label: fig-bart-sim1
#| fig-cap: Visualising the predictions from BART, we see particularly peculiar predictions around $t=0.05$. Note that the green points are from the training data, yet the mean appears well below. The non-additive noise in the generating model violates BART's assumption of normally distributed errors. Using a highly informative, misspecified prior on the standard deviation stretches the posterior predictive distribution to nominal coverage. As a result, the mean is pulled away from local observed values. The hyperparameter values used here are $\hat\sigma=2SD(y),\ \lambda=220,\ k=0.5,\ \alpha=0.99, \beta=0.5,\ q=0.99$. A total of 250 trees were used.
#| echo: false

ggplot() +
  geom_ribbon(aes(x = i, ymin = pred_quants[, 1], ymax = pred_quants[, 2],
                  fill = "95% PPI"), alpha = 0.5) +
  geom_line(aes(x = i, y = colMeans(pred), color = "BART Mean")) +
  geom_line(aes(x = i, y = znn, color = "True Function"), alpha = 0.9) +
  geom_point(aes(x = i, y = z), alpha = 0.2, color = "darkgreen", data = true0) + 
  quant_theme +
  theme(
    legend.position = c(0.8, 0), 
    legend.spacing.x = unit(0.1, "cm"),
    legend.box = "horizontal"
  ) +
  scale_color_manual(
    values = c("BART Mean" = "magenta", "True Function" = "darkgreen"), 
    name = ""
  ) +
  scale_fill_manual(
    values = c("95% PPI" = "gold"),
    name = ""
  ) +
  labs(y = "y", x = "t", title = "BART Fit Example")
```

Ensembles of trees are not used exclusively for prediction. Two inferential tools that are commonly used with both random forests and BART are variable importance and variable interaction measures. We briefly describe these measures, using \citep{bartMan} bartMan R package. Notes on variable importance and variable interaction measures are based on the documentation. Similar measurements have been useful for inference from random forests, but the calculations are different.

Variable importance can be measured by looking at the proportion of decisions in which a variable is used. For BART, the structure of the trees varies across iterations of MCMC, so we define $c_{r,k}$ as the number of times variable $r$ is used as a splitting decisions within the $k'$th posterior sample. $c_k=\sum_{j=1}^Pc_{r,k}$ is thus the total number of decisions for the $k'$th posterior sample. Variable importance is then measured as $\text{VImp}_r=\sum_{k=1}^K\frac{c_{r,k}}{c_k}$. Thinking of the extreme cases, with covariates that provide no information about the response, we should simply estimate $f$ as the mean of the observed data. BART will make few decisions in this scenario as proposals will provide little to no improvement to the likelihood. If we had some variable that completely explained our response, BART may consistently make decisions off values of said feature.

Similar to how variable importance is measured, we can assess the potential for interactions by calculating the proportion of successive decisions concerning two variables across posterior samples. Individual decisions/sequences of decisions do not have much meaning on there own, making these algorithms hard to explain. It makes sense though that if a variable is consistently used, it must be able to explain the response to some degree. These tools can be used to interpret a BART model, or to help with variable selection for another model. @fig-vimp_vint displays a matrix of variable importance and interaction measures for data simulated under as $Y=X_1(X_2 + X_3+X_6)+X_2\cdot X_6+\sin(X_1)+\cos(X_2)+\log(|X_3|)+X_4^2+\lfloor X_5\rfloor+\lceil X_6\rceil+\epsilon$. We see all interactions are found, but the plot suggests some evidence of an interaction between $X_3$ and $X_6$. The coefficient of variation is a measure used to compare one variable to another and does not have much meaning on it's own. Like any other measure in statistics, results must be scrutinized.

![The matrix of coefficient of variation values for variable importance and interaction measures shows BART makes reasonable selection decisions in a contrived example. Variable importance and interaction measures are useful tools for inference and exploratory analysis even if they must be scrutinized.](figures/vimp_pres.png){#fig-vimp_vint}

Before we move on to the data, we briefly discuss diagnostics for BART. Unlike the random forest algorithm, a BART model for a continuous response assumes normally distributed errors and constant variance. When using a BART model, we need to assess the normality assumption, model fit, and convergence. The bartMan R package \citep{bartMan} provides a convenient tool for diagnostics. @fig-bart_diag shows the diagnostics for the same model used for @fig-vimp_vint. The Q-Q plot and Histogram both indicate normality was reasonable as expected. We see constant variance is reasonable as well in the Fitted vs Residuals Plot. The actual vs fitted plot shows BART fits the data quite well as the observed and fitted values are quite close. Variable Importance intervals shows BART correctly identified all variables as important as the intervals do not contain zero. The trace plot for $\sigma$ shows the standard deviation has converged reasonably well. BART models have many parameters, and it is not unheard of to avoid assessing the convergence of each tree. The thought behind this is we don't care about any of the individual trees, so as long as our predictions have converged the model fits reasonably well. Changing the number of trees, along with $\alpha$ and $\beta$ can help with convergence.

![Diagnostic plots for BART show the model easily handles interactions and non-linear relationships. Like any other regression model, our inferences will be influenced by how well the model fits the data.](figures/bart_diag.png){#fig-bart_diag}

```{python}
#| eval: false
#| echo: false
#| include: false


import ee
import geemap
import webbrowser
import matplotlib.pyplot as plt
import matplotlib as mpl


ee.Authenticate()
ee.Initialize(project='ee-bdevries252')

bb_long = 153.5557
bb_lat = -28.8371
bb_point = ee.Geometry.Point(bb_long, bb_lat)
lis_long = 153.1536
lis_lat = -28.4941
lis_point = ee.Geometry.Point(lis_long, lis_lat)
diff = [bb_long - lis_long, lis_lat - bb_lat]
mid_point = ee.Geometry.Point((bb_long + lis_long) / 2, (bb_lat + lis_lat) / 2)
mp_coords = mid_point.coordinates().getInfo()
bb_aoi = ee.Geometry.Rectangle([bb_long - 0.1, bb_lat - 0.1,
                                bb_long + 0.1, bb_lat + 0.1])

modis = ee.ImageCollection('MODIS/061/MCD12Q1')
modis = modis.filterBounds(bb_aoi)
aumd = modis.select("LC_Type2").sort('system:time_start', False).first()

task = ee.batch.Export.image.toDrive(
    image = aumd,
    description = 'MODIS_LandCover',
    fileNamePrefix ='modis_landcover2023',
    region = bb_aoi,  
    scale = 500,
    crs='EPSG:7844',
    maxPixels=1e13
)
task.start()

built_im = ee.Image('JRC/GHSL/P2023A/GHS_BUILT_S/2020')
built_rast = built_im.select("built_surface")
task = ee.batch.Export.image.toDrive(
    image = built_rast,
    description = 'GHSL_built_surface2023',
    fileNamePrefix ='ghsl_built_surface2023',
    region = bb_aoi,  
    scale = 500,
    crs='EPSG:7844',
    maxPixels=1e13
)
task.start()

era5 = ee.ImageCollection('ECMWF/ERA5_LAND/MONTHLY_BY_HOUR')
era5 = era5.filterBounds(bb_aoi).filterDate(ee.Date('2023'))
soil_im = era5.select("soil_temperature_level_1").first()
task = ee.batch.Export.image.toDrive(
    image = soil_im,
    description = 'era5_2023_soil_temp',
    fileNamePrefix ='era5_soil_temp',
    region = bb_aoi,  
    scale = 500,
    crs='EPSG:7844',
    maxPixels=1e13
)
task.start()

precip_im = era5.select('total_precipitation').first()
task = ee.batch.Export.image.toDrive(
    image = precip_im,
    description = 'era5_2023_precip',
    fileNamePrefix ='era5_precip',
    region = bb_aoi,  
    scale = 500,
    crs='EPSG:7844',
    maxPixels=1e13
)
task.start()


dems = ee.Image("AU/GA/DEM_1SEC/v10/DEM-H")
elevation = dems.select("elevation")
task = ee.batch.Export.image.toDrive(
    image = elevation,
    description = 'balina_byron_elev',
    fileNamePrefix = 'balina_byron_elev',
    region = bb_aoi,  
    scale = 30.92,
    crs='EPSG:7844',
    maxPixels=1e13
)
task.start()

hansen_forest = ee.Image('UMD/hansen/global_forest_change_2023_v1_11').select('loss')
task = ee.batch.Export.image.toDrive(
    image = hansen_forest,
    description = 'forest_loss',
    fileNamePrefix = 'forest_loss',
    region = bb_aoi,  
    scale = 30.92,
    crs='EPSG:7844',
    maxPixels=1e13
)
task.start()
```

# Data {#sec:Data}

Before we can apply any of the models discussed to assess the importance of land cover on flying fox feeding locations, we need to extract and transform the data. The flying fox locations themselves form a point pattern, which can't be modeled directly by any of the algorithms discussed. Point patterns are a realization of a point process, defined by a random number of observations in random locations. Modelling point pattern data requires estimating the mean occurrence rate across an entire surface. We're not particularly interested in the precise locations flying foxes feed so modelling a point pattern is overkill. Instead, we focus on the land cover characteristics of larger grid cells to examine the potential association with feeding locations. In this section we discuss the tools used to obtain and process our data.

```{python}
#| include: FALSE
#| echo: FALSE
#| eval: FALSE
import ee
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

ee.Authenticate()
ee.Initialize(project = 'ee-bdevries252')

dat = pd.read_csv('/home/benjamin/DeVries_WP/hull_final.csv')
bb_poly = ee.Geometry.Polygon([dat[['x', 'y']].values.tolist()])
bb_box = ee.Geometry.BBox(min(dat['x']), min(dat['y']), max(dat['x']), max(dat['y']))
bb_grid = bb_box.coveringGrid('EPSG:4326', np.sqrt(bb_poly.area().getInfo() / 4000))
bb_aoi = bb_grid.filterBounds(bb_poly)


coordinates = bb_poly.getInfo()['coordinates'][0]
x, y = zip(*coordinates)
plt.plot(x, y, color='blue')
grid_cells = bb_aoi.getInfo()['features']
for grid_cell in grid_cells:
    grid_coords = grid_cell['geometry']['coordinates'][0]
    grid_x, grid_y = zip(*grid_coords)
    plt.plot(grid_x + (grid_x[0],), grid_y + (grid_y[0],), color = 'red', linewidth = 0.5)
plt.grid(True)
plt.show()

def areaCells(cell):
    return cell.set('area_m2', ee.Geometry(cell.geometry()).area(ee.Number(1)))

bb_aoi = bb_aoi.map(areaCells)

task = ee.batch.Export.table.toDrive(
    collection = bb_aoi,
    description = 'writing_project_aoi',
    fileNamePrefix = 'bb_grid_5000',
    fileFormat = 'CSV'
)
task.start()

#csv
START = ee.Date('2023-08-01')
END = START.advance(1, 'year')
col_filter = ee.Filter.date(START, END)
band_names = ['water','trees','grass','flooded_vegetation','crops','shrub_and_scrub','built','bare']
dw_col = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1').filter(col_filter)

for band in band_names:
    img = dw_col.select(band).median()
    zonal = img.reduceRegions(collection = bb_aoi, reducer = ee.Reducer.mean(), scale = 10)
    task = ee.batch.Export.table.toDrive(
        collection = zonal,
        description = f'DW_mean_{band}',
        fileNamePrefix = f'dynamic_world_mean_{band}',
        fileFormat = 'CSV'
    )
    task.start()
    print(f"Exporting: DW_mean_{band}")
# rasters

START = ee.Date('2023-08-01')
END = START.advance(1, 'year')

col_filter = ee.Filter.And(
    ee.Filter.bounds(ee.Geometry.Point(bb_long, bb_lat)),
    ee.Filter.date(START, END),
)
band_names = ['water', 'trees', 'grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built', 'bare']
dw_col = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1').filter(col_filter)
dw_ims = [ee.Image() for _ in range(8)]

for i in range(8):
  dw_ims[i] = dw_col.select(band_names[i]).median()
  
index = 0
for band in band_names:
    img = dw_ims[index]
    task = ee.batch.Export.image.toDrive(
        image=img.clip(bb_aoi),
        description=f'DW_{band}',
        fileNamePrefix=f'dynamic_world_{band}',
        region=bb_aoi.getInfo()['coordinates'],
        scale=10,
        maxPixels=1e13
    )
    task.start()
    print(f"Exporting: DW_{band}")
    index += 1

```

```{r}
#| include: FALSE
#| echo: FALSE
#| eval: FALSE

dw_files <- list.files(path = "~/Downloads/", pattern = "^dynamic_world.*\\.tif$",
                       full.names = T)
rast_dat <- as.data.frame(raster(dw_files[1]), xy = T)
vals <- apply(matrix(2 : length(dw_files)), 1, function (a) {
  rast <- raster(dw_files[a])
  dat <- as.data.frame(rast, xy = T)
  return(dat[, 3])
})
rast_dat <- cbind(rast_dat, vals)
rast_dat <- rast_dat |>
  rename("Bare" = bare,
                "Built" = `1`,
                "Crops" = `2`,
                "Flooded Veg." = `3`,
                "Grass" = `4`,
                "Shrub/Scrub" = `5`,
                "Trees" = `6`,
                "Water" = `7`) |> 
  pivot_longer(cols = -c(x, y), names_to = "layer", values_to = "value")
ggplot(data = rast_dat, aes(x = x, y = y, fill = value)) +
  geom_raster() +
  scale_fill_viridis_c() +
  facet_wrap(~ layer) +
  theme_void() +
  labs(x = "", y = "", fill = "Estimated Probability",
       title = "Dynamic World Land Cover Rasters 2022 Median Estimated Probability Near Ballina Byron") +
  guides(fill = guide_colorbar(direction = "horizontal", title.position = "top")) +
  theme(
    legend.position = c(0.7, 0),
    legend.justification = c(0, 0),
    legend.key.size = unit(0.9, "cm"),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )
```

## Raster Data and Google Earth Engine

Rasters are used to store data with spatial components. Each pixel has an $(x,y)$ coordinate, and a variable of interest. We can visualize rasters by mapping the variable to a color. Gradient scales can be used for continuous values. For this project, we use rasters pulled via Google Earth Engine (GEE), \citep{gee}. GEE has a large database of satellite images and rasters of estimated attributes. Many earth engine datasets contain images describing different attributes of the land. These attributes are labelled bands.

An important component of spatial rasters is the coordinate reference system (CRS). We can think about the satellite images as pictures of a sphere. Using the position of the camera and other factors such as distance to the earth, $(x,y,z)$ coordinates can be assigned to each pixel. For spatial data, we need to know how far apart locations are on the sphere. Looking at a picture of a sphere, we can't determine how far apart points are due to the curvature. Gauss's Theorema Egregium implies that no 2d map perfectly portrays the distance between points. We need to choose a CRS that preserves the distance between points for our area of interest reasonably well. The location on the earth and size of the area determine this choice. Each raster has a pre-specified CRS that may or may not be suitable, but rasters can always be projected into another CRS.

Working with data from earth engine we can either select an individual image, or an image collection. An image corresponds to a specific band and time, while the image collection is the entire data set. When either is selected, we have access to every time and location available. With an image, we can clip area of interest, masking data outside this region. With an image collection, we can filter the dataset to a specific region and time period. After selecting a band, an individual image can be chosen. Alternatively, we can produce an image based calculation over the filtered time period and region. Images can then be exported to Google Drive and loaded into R with the raster package, \citep{raster}. After loading the raster, we can create a long formatted data structure for modelling.

## Land Cover and Dynamic World

One category of datasets on Google Earth Engine is land cover. Land cover describes natural and developed features of the earth as classes. A similar but distinct class of datasets is land use, which describes how humans use the land. There are various methods to estimate land cover, and many models estimate different classes. Additionally, different datasets span various regions and time periods. The majority of land cover datasets are annual and not quite up to date. Estimates are produced in a multi-phase modelling process, considering many images along with external prior information. Working with recent data from Australia, there are fewer options, especially after excluding low resolution datasets. In this project, we utilize data from DynamicWorld V1 by \cite{dynamic_world}. Dynamic World is a daily high resolution land cover dataset derived from Sentinel-2 satellites. The estimated probability of each class in dynamic world is the output from a convectional neural nets predictions on a single satellite image.

Within Dynamic World there are ten bands, each containing a collection of images at a $10\ m$ resolution. In our analysis, we use eight relevant bands that each contain the estimated probability that a $100\ m^2$ region of the earth corresponds to the chosen band. Unsurprisingly, daily high resolution data is rife with missingness. To combat this, we compute the median land class over a selected period of time. Missing values are ignored in the calculation by default. This also helps slightly with the fact that Dynamic World estimates are based on an individual day. Weather and atmospheric factors may influence estimates so looking at the closest day may not be advisable. We don't expect major changes to land cover in a short period of time so this is reasonable. An individual image can then be produced based on some function of the images. A panel of the exported rasters is displayed in @fig-dw-lc.

![Dynamic World Exports from Google Earth Engine](figures/2022_rast.png){#fig-dw-lc}

```{python}
#| eval: FALSE
#| echo: false

bb_long = 153.5557
bb_lat = -28.8371
bb_aoi = ee.Geometry.Rectangle([bb_long - 0.2, bb_lat - 0.2,
                                bb_long + 0.2, bb_lat + 0.2])
START = ee.Date('2022-01-01')
END = START.advance(1, 'year')

col_filter = ee.Filter.And(
    ee.Filter.bounds(ee.Geometry.Point(bb_long, bb_lat)),
    ee.Filter.date(START, END),
)
band_names = ['water', 'trees', 'grass', 'flooded_vegetation', 'crops', 'shrubs_and_scrub', 'built', 'bare']
dw_col = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1').filter(col_filter)
dw_ims = [ee.Image() for _ in range(8)]

for i in range(8):
  dw_ims[i] = dw_col.select(band_names[i]).median().clip(bb_aoi)
  
index = 0
for band in band_names:
    img = dw_ims[index]
    task = ee.batch.Export.image.toDrive(
        image = img,
        description = f'DW_{band}',
        fileNamePrefix = f'dynamic_world_{band}',
        region=bb_aoi.getInfo()['coordinates'],
        scale=10,
        maxPixels=1e13
    )
    task.start()
    print(f"Exporting: DW_{band}")
    index += 1
```

```{r}
#| eval: FALSE
#| echo: FALSE
#| include: FALSE

modis <- raster("~/Downloads/modis_landcover2023.tif")
modis_df <- as.data.frame(modis, xy = T) |> 
  mutate(land_cover = factor(LC_Type2, levels = c(0, 1, 2, 5, 8, 9, 10, 12, 13),
                             labels = c("Water Bodies",
                                        "Evergreen Needleleaf Forests",
                                        "Evergreen Broadleaf Forests",
                                        "Mixed Forests",
                                        "Woody Savannas",
                                        "Savannas",
                                        "Grasslands",
                                        "Croplands",
                                        "Urban and Built-up Lands")))
modis_pal <- c(
  "Water Bodies" = "#1c0dff",
  "Evergreen Needleleaf Forests" = "#05450a",
  "Evergreen Broadleaf Forests" = "#086a10",
  "Deciduous Broadleaf Forests" = "#78d203",
  "Mixed Forests" = "#009900",
  "Closed Shrublands" = "#c6b044",
  "Open Shrublands" = "#dcd159",
  "Woody Savannas" = "#dade48",
  "Savannas" = "#fbff13",
  "Grasslands" = "#b6ff05",
  "Croplands" = "#c24f44",
  "Urban and Built-up Lands" = "#a5a5a5",
  "Non-Vegetated Lands" = "#f9ffa4"
)
ggplot() +
  geom_raster(aes(x = x, y = y, fill = land_cover),
              data = modis_df) +
  scale_fill_manual(name = "Land Cover", values = modis_pal) +
  geom_point(aes(x = 153.5557, y = -28.8371, color = "Ballina Byron"), size = 5) +
  scale_color_manual(name = "Airport", values = c("magenta")) +
  labs(title = "MODIS Land Cover Raster for Eastern Australia 1/1/2023",
       x = "Longitude", y = "Latitude") +
  theme_minimal()
# 
# soil_temp <- raster("~/Downloads/era5_soil_temp.tif")
# soil_temp_df <- as.data.frame(soil_temp, xy = T)
# ggplot() +
#   geom_raster(data = soil_temp_df,
#               aes(x = x, y = y, fill = soil_temperature_level_1)) +
#   scale_fill_viridis_c()
# 
# precip_im <- raster("~/Downloads/era5_precip.tif")
# precip_df <- as.data.frame(precip_im, xy = T)
# ggplot() +
#   geom_raster(data = precip_df,
#               aes(x = x, y = y, fill = total_precipitation)) +
#   scale_fill_viridis_c()
# 
# elev_im <- raster("~/Downloads/balina_byron_elev.tif")
# elev_df <- as.data.frame(elev_im, xy = T)
# ggplot() +
#   geom_raster(aes(x = x, y = y, fill = elevation), data = elev_df) +
#   scale_fill_viridis_c()
# 
# forest_loss <- raster("~/Downloads/forest_loss.tif")
# forest_loss_df <- as.data.frame(forest_loss, xy = T)
# ggplot() +
#   geom_raster(aes(x = x, y = y, fill = elevation), data = elev_df, alpha = 0.8) +
#   scale_fill_viridis_c() +
#   geom_contour(aes(x = x, y = y, z = loss), data = forest_loss_df, color = "orange")
```

## Data Processing

The radar data used in our project was extracted from an archived PostgreSQL database with three days of observations. The radar itself is recording data day and night, and sees many objects and organisms other than bats. Birds and bats are essentially the same to the radar. Our first step in processing the data was to filter observations by time and class. Using SQL, we construct a table of flight paths for the three sizes (classes)
of bat where the last observed location is recorded between 12:00 am and 5:00 am. The flight paths are recorded as linestrings, a geometry representing the path through a sequence of locations.

Using our table of linestrings, we define an area of observation for the bats with the ST_ConcaveHull from PostGIS. The concave hull function takes a geometry or table of geometries, and shrink wraps a polygon around them. There is a parameter to control the convexity, which we set to 0.99, allowing minimal concavity. The polygon returned by ST_ConcaveHull along with the final observed location of bats were then exported for additional data processing.

Any form of spatial data can be transformed into areal data by defining a grid and aggregating observations within the cells. Before aggregating flying fox counts, we need to define a grid. Returning to Google Earth Engine, the previously defined boundary is imported, an enclosing rectangle defined. The coveringGrid function is then used to define our grid. The grid can be filtered based on our area of interest. Using the filtered grid we, sample pixel values from dynamic world to obtain the mean probability for each class over the grid. Resampling in earth engine is much easier than manually averaging as we don't have to worry about weighting averages when grid cells don't align with the cells of rasters. Without paying additional fees, the number of cells that can be resampled is capped at 5000. For a larger area, the grid could be partitioned into blocks to resample in batches. At a scale of $289.6\ m$, our grid contains 4748 observed locations. There is slight variation in the size of cells, ranging from $73,212$ to $73,381$ $m^2$.

Our gridded area of interest was then imported into R along with the three nights of radar data. Using the lengths and st_intersect function from the sf package \citep{sf}, we can easily obtain flying fox counts for each grid cell. @fig-rast-proces shows our processed data rasterized.

![The correlation matrix indicates flying fox counts have weak to moderate correlation with land the various classes of land cover. It's still possible that our model will be able to make good predictions as interactions and non-linear behavior can make correlation missleading.](figures/proc_rast.png){#fig-rast-proces}

```{r}
#| eval: false
#| include: false
#| output: false
proc_dat <- read_csv("~/DeVries_WP/dw_4748.csv") |> rename(Count = count)
dat_long <- proc_dat |> 
  rename_with(
    .cols = starts_with("prob_"),
    .fn = ~ str_to_title(str_remove(., "^prob_"))
  ) |> 
  pivot_longer(
    cols = 1 : 9,
    names_to = "variable",
    values_to = "value"
  ) |> 
  mutate(scale_group = if_else(variable == "Count", "count", "prob"))

ggplot() +
  geom_tile(
    data = filter(dat_long, scale_group == "count"),
    aes(x = lon, y = lat, fill = value)
  ) +
  scale_fill_viridis_c(option = "D", name = "Count") +
  ggnewscale::new_scale_fill() +
  geom_tile(
    data = filter(dat_long, scale_group == "prob"),
    aes(x = lon, y = lat, fill = value)
  ) +
  scale_fill_viridis_c(option = "A", name = "Probability") +
  facet_wrap(~ variable, ncol = 3) +
  coord_equal() +
  theme_minimal() +
  labs(title = "Processed Dynamic World and Flying Fox Data", x = "", y = "")
```


```{r}
#| echo: false
#| output: false
#| eval: false

dat_228 <- read_csv("~/DeVries_WP/all_bat.csv")
extracted <- apply(matrix(dat_228[, 4]), 1, function(a) {
  return(str_extract_all(a, "-?\\d+\\.\\d+|-?\\d+")[[1]])
}) |> as.numeric() |>  as.data.frame()
colnames(extracted) <- "val_type"
extracted$row_id <- rep(1 : nrow(dat_228), each = 4)
extracted_wide <- cbind(extracted, rep(c("Long", "Lat", "Val1", "Val2"),
                                       nrow(dat_228))) |> 
  pivot_wider(
    id_cols = row_id,
    names_from = `rep(c(\"Long\", \"Lat\", \"Val1\", \"Val2\"), nrow(dat_228))`,
    values_from = val_type
  )
grid_ee <- read_csv("~/DeVries_WP/bb_grid_5000.csv")
grid_ee_sf <- geojson_sf(grid_ee$.geo)
bats_sf <- st_as_sf(extracted_wide[, 2 : 3], coords = c("Long", "Lat"))
st_crs(bats_sf) <- st_crs(grid_ee_sf)
grid_ee_sf$count <- lengths(st_intersects(grid_ee_sf, bats_sf))
```

```{r}
#| echo: false
#| output: false
#| eval: false

adjacency_list <- st_touches(grid_ee_sf)
n <- nrow(grid_ee_sf)
adjacency_matrix <- matrix(0, n, n)
for (i in seq_along(adjacency_list)) {
  adjacency_matrix[i, adjacency_list[[i]]] <- 1
}
write_csv(adjacency_matrix |> as.data.frame(), "~/DeVries_WP/adj_mat.csv")
```


```{r}
#| echo: false
#| output: false
#| eval: false

ggplot(grid_sf) +
  geom_sf(aes(fill = count), color = "white") +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(fill = "Bat Count")

ggplot() +
  geom_raster(aes(x = x, y = y, fill = water), data = rast_dat) +
  scale_fill_viridis_c() +
  ggnewscale::new_scale_fill() +
  geom_polygon(aes(x = x, y = y), fill = NA, colour = "black", data = hull_dat) +
  geom_point(aes(x = Long, y = Lat), color = "red", data = extracted_wide,
             alpha = 0.2)
```

# Modelling and Results

## Exploratory Data Analysis

Our modelling process begins with a brief exploratory data analysis. While we are not specifying a parametric relationship between flying fox counts and land cover, exploratory data analysis may provide insights that help interpret results. A correlation matrix for the dynamic world data and flying fox counts was created with the ggcorrplot function, \citep{catstat2} and is displayed in @fig-cor_mat. Weak to moderate correlation is seen between flying fox counts and Built, trees, water, and shrub. Water shows a negative correlation with flying fox counts, which can be explained by flying foxes travelling to islands. It shouldn't be hard for BART to figure out that bats will not feed in the ocean as the estimated land cover probabilities other than water are quite low in these areas. While several of our covariates display fairly low correlations with flying fox counts, non-linear relationships and interactions may make features such as bare relevant.

![The correlation matrix indicates flying fox counts have weak to moderate correlation with land the various classes of land cover. It's still possible that our model will be able to make good predictions as interactions and non-linear behavior can make correlation missleading.](figures/cor_mat.png){#fig-cor_mat}

To further explore the potential associations between land cover and flying fox counts, we plot several combinations of correlated predictors in @fig-scatter. Zero counts were removed to improve visibility of associations in characteristics of feeding locations. Plots (B), (C), (D), (F), and (G) show clusters of large points away from the edges, providing some evidence of potential interactions. Concluding our exploratory data analysis, we have preliminary evidence that water, flooded areas, trees, shrubs, and developed areas have some association with flying fox ocunts.

![Scatter plots to show potential two-way interactions of land cover classes. Looking towards the center of each plot, large dots indicate a potentially usefult interaction for predictions at the most popular areas. There is some evidence of trees interacting with water, shrubs, and flooded vegation. Water potentially interacts with grass as well.](figures/scatter.png){#fig-scatter}

## Predictions and Diagnostics

Prior to fitting any BART models, we created a 70/30 training/testing split. An initial BART model was fit with default the default parameters and hyperparameters. Convergence was assessed with a trace plot for $\sigma$. The burn in period was increased by 50,000 samples at a time until $\sigma$ had stabilized. The initial model converged reasonably well after a burn in period of 200,000 with 50,000 total draws. Continuous rank probability scores (CRPS) obtained from the crps_sample function from the scoringRules \citep{scoringRules} package are displayed in @fig-prelim. We see low CRPS values indicating accurate, precise predictions on the right hand side of our area of interest and nearly nowhere else. The model was able to predict that bat's are not likely to feed in the ocean, but not much else. This makes some sense as even if an area is abundant with food, flying foxes may not travel there because resources are available somewhere closer. Results from our initial model indicate land cover alone cannot predict feeding locations.

![We see the coastline and everywhere to the left has fairly light colors, indicating poor predictions. Without considering the energy required to travel to the edges of our area of intest, the model struggles to discern what land characteristics flying foxes are drawn to.](figures/prelim_res.png){#fig-prelim}

We need some way to account for the additional energy expended in travelling to the edges of the region of interest. Using the National Flying Fox Monitor Viewer \citep{ff_view}, we obtained coordinates for the largest flying fox camp within our observed area and calculated the distance to each of our grid cells. Other camps within the region are smaller and located nearby, suggesting additional distance to camp covariates would add little information. With the distance to camp as a covariate, we refit our BART model. A trace plot showing $\sigma$ has converged reasonably well is displayed in @fig-trace. The trace shows some variation in the vertical position of the band of samples, but after 200,000 samples, there is no clear trend. The newly fitted BART model was then evaluated on the testing data.

![Earlier samples tended to decrease with iterations of MCMC, so a large burn in period was used. Even with some variation in the green band of samples, 50,000 draws should give a posterior that is characterized reasonably well for inference. Our trace plot for $\sigma$ gives evidence the model has converged, but does not guarantee the posterior predictive distribution has converged at each site. This is unlikely to impact inference and was thus ignored.](figures/trace.png){#fig-trace}

Using the crps_sample function, we evaluated our model that accounts for distance to the camp. @fig-final_crps displays the CRPS values at testing sites along with the observed counts for training data. Again, our model's best predictions essentially say flying foxes will not disappear into the ocean. While the model clearly struggles to predict the highest flying fox counts, we see darker cells within the bright blue blob where the majority of flying foxes feed. This suggests BART was able to learn some features associated with foraging locations. With such a large range of CRPS values, it's difficult to estimate values visually. A table of summary statistics for the CRPS values is displayed in @tbl-crps_summary.

![The CRPS scores along with observed counts show our model has some ability to predict flying fox feeding locations out of sample. Model misspecification may exacerbate high CRPS values at larger counts. In count distributions, the variance increases with the mean. This makes violations of the assumptions of constant variance and normally distributed errors probable.](figures/pred_final.png){#fig-final_crps}

|   |   min|   Q1| median|     Q3|      max|    mean|   $\sigma$|
|:--|-----:|----:|------:|------:|--------:|-------:|----------:|
|   | 0.108| 0.39|  1.235| 36.377| 4463.611| 127.127|  356.402  |

: Summary statistics for CRPS values. While some predictions are far off, most are reasonable. About 60% of grid cells have zero counts, suggesting the model is good at identifying where flying foxes donâ€™t feed. {#tbl-crps_summary}

To further assess our models ability to predict foraging sites, we present histograms of the observed and predicted counts in @fig-hist. Histograms were chosen because we treat counts as continuous. Our BART model predicts large negative values for several locations. Log plus one and log plus $\epsilon$ ($\epsilon$ small) transformation for the response were considered, but presented there own issues. The log plus one transformation exacerbated poor predictions at sites with large counts. The log plus $\epsilon$ transformation makes zero counts very distinct from single counts. With our goal of discerning feeding locations, the response was left untransformed. For nonzero counts, the posterior predictive distribution is reasonably close to the observed counts. Generally, our model underestimates the number of counts within each bin. A truncated BART model would likely better reflect the data, as the probability of counts below zero would be added to observable values. Some of our larger CRPS values can be attributed to model misspecification. 

![Other than the large left tail below zero, the histograms of the posterior predictive distribution for the testing data and observed counts are fairly similiar in shape. This indicates BART makes mostly reasonable predictions, but does not say anything about the predictions at individual sites.](figures/pp_hist.png){#fig-hist}

After assessing our models predictive capabilities, we checked model diagnostics with the Q-Q and residual vs fitted values plots displayed in @fig-qq. Mean predictions were used instead of the full posterior for computational efficiency. The Q-Q plot shows very heavy tails, indicating a severe violation of our assumption of normality. In the residuals vs fitted values plot, we see a distinct slanted line of residuals due to counts being non-negative. This creates issues with the assumption of a linear relationship between $\mu_{y}$ as large negative predictions create strictly positive residuals. This in turn forces overestimation of larger predicted values as residuals are assumed to have mean zero. Our assumption of constant variance appears fairly reasonable as the vertical spread of residuals is fairly consistent. Overall, model diagnostics appear very poor, decreasing our confidence in inferences drawn from the model.

![Our residual diagnostic plots show clear violations of normality and linearity. Even without plotting the full posterior residuals, we can tell our model is miscallibrated. Further results must be taken with a grain of salt.](figures/diag_final.png){#fig-qq}

Keeping our models limitations in mind, we present a series of violin plots for variable inclusion probabilities in @fig-inc_prob. Distance to the flying fox camp is by far the most important factor. Intuitively this makes sense as animals tend to find the path of least resistance. Human developed areas (built), shrub, and tree land cover classes display the next highest inclusion probabilities. We can infer a positive association with flying fox counts and trees as fruit bats forage in trees, and the radar cannot track objects below tree canopy. Flooded vegetation, grass, and water also have relatively high inclusion probabilities. The Bare land and crop classes display relatively low variable inclusion probabilities, but the distributions do not include zero implying they were consistently used. The Dynamic World bare and crop classes display fairly low probabilities across the area of interest, so it makes sense that they will be used in fewer decisions. Variable inclusion probabilities are a purely relative measure, so it is difficult to say whether any of the classes are truly associated with foraging sites. Confounding variables are certainly a concern but we can draw some faith from the fact that a class we expect to have some association (trees), displays a similar inclusion probability to the majority of the other classes.

![Violin plots of variable inclusion probabilities show the distance to the flying fox camp is most important. Bare land and crops play the least significant role while all other variables have fairly similiar inclusion probabilities. We expect trees to have some association based on background knowledge. Similiar inclusion probabilities for other classes provide some evidence of associations with flying fox counts.](figures/inc_prob.png){#fig-inc_prob}

```{r}
#| echo: false
#| output: false
#| eval: false

dat <- read_csv("./dw_4748.csv")[, 1 : 9]
samp_i <- sample(1 : nrow(dat), 500, replace = F)
samp_dat <- dat[samp_i, ]
colnames(samp_dat) <- colnames(dat) <-  c("Count", "Bare", "Built", "Crops",
                                          "Flooded",
                                "Grass", "Shrub", "Trees", "Water")
long_samp <- samp_dat |>
  pivot_longer(cols = -Count, names_to = "Variable", values_to = "Value")

ggplot(data = long_samp) +
  geom_point(aes(x = Value, y = Count, color = Variable), alpha = 0.5) +
  geom_smooth(aes(x = Value, y = Count, color = Variable)) +
  facet_wrap(~ Variable) +
  theme_minimal() +
  guides(color = "none")

p0 <- ggplot(dat |> filter(Count != 0)) +
  geom_point(aes(x = Water, y = Built, alpha = Count, size = Count),
             color = "black", fill = "orange", pch = 21) +
  scale_alpha_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  scale_size_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  theme_minimal() +
  guides(alpha = "none", size = "none")
p1 <- ggplot(dat |> filter(Count != 0)) +
  geom_point(aes(x = Water, y = Trees, alpha = Count, size = Count),
             color = "black", fill = "orange", pch = 21) +
  scale_alpha_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  scale_size_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  theme_minimal() +
  guides(alpha = "none", size = "none")
p2 <- ggplot(dat |> filter(Count != 0)) +
  geom_point(aes(x = Shrub, y = Trees, alpha = Count, size = Count),
             color = "black", fill = "orange", pch = 21) +
  scale_alpha_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  scale_size_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  theme_minimal() +
  guides(alpha = "none", size = "none")
p3 <- ggplot(dat |> filter(Count != 0)) +
  geom_point(aes(x = Shrub, y = Water, alpha = Count, size = Count),
             color = "black", fill = "orange", pch = 21) +
  scale_alpha_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  scale_size_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  theme_minimal() +
  guides(alpha = "none", size = "none")
p4 <- ggplot(dat |> filter(Count != 0)) +
  geom_point(aes(x = Crops, y = Water, alpha = Count, size = Count),
             color = "black", fill = "orange", pch = 21) +
  scale_alpha_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  scale_size_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  theme_minimal() +
  guides(alpha = "none", size = "none")
p5 <- ggplot(dat |> filter(Count != 0)) +
  geom_point(aes(x = Grass, y = Water, alpha = Count, size = Count),
             color = "black", fill = "orange", pch = 21) +
  scale_alpha_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  scale_size_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  theme_minimal() +
  guides(alpha = "none", size = "none")
p6 <- ggplot(dat |> filter(Count != 0)) +
  geom_point(aes(x = Flooded, y = Trees, alpha = Count, size = Count),
             color = "black", fill = "orange", pch = 21) +
  scale_alpha_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  scale_size_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  theme_minimal() +
  guides(alpha = "none", size = "none")
p7 <- ggplot(dat |> filter(Count != 0)) +
  geom_point(aes(x = Trees, y = Bare, alpha = Count, size = Count),
             color = "black", fill = "orange", pch = 21) +
  scale_alpha_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  scale_size_continuous(breaks = c(1, 100, 1000, 4000, 8000)) +
  theme_minimal()
ggarrange(p0, p1, p2, p3, p4, p5, p6, p7,
          labels = c("(A)", "(B)", "(C)", "(D)", "(E)", "(F)", "(G)", "(H)")) +
  ggtitle("Scatter Plots of Estimated Probabilities with Counts")
```
```{r}
#| echo: false
#| output: false
#| eval: false

cor(dat) |> catstats2::ggcorrplot(lab = T, type = "lower") +
  theme(axis.line = element_blank(), line = element_blank()) +
  labs(title = "Correlation Matrix for Land Cover and Flying Fox Counts")
```
```{r}
#| echo: false
#| output: false
#| eval: false

dw_files <- list.files(path = "~/Downloads/dw_rasts/",
                       pattern = "^dynamic_world.*\\.csv$", full.names = T)
type <- c("bare", "built", "crops", "flooded", "grass", "shrub", "trees", "water")

for (i in 1 : length(dw_files)) {
  tmp <- read_csv(dw_files[i])
  colnames(tmp) <- c("index", paste0("area_", type[i]), paste0("prob_", type[i]),
                     "geo")
  grid_ee_sf <- cbind(grid_ee_sf, tmp[, 2 : 3])
}
grid_ee_sf$density <- grid_ee_sf$count / grid_ee_sf$area_bare
grid_ee_sf <- grid_ee_sf |> 
  mutate(centroid = st_centroid(geometry)) |> 
  mutate(
    lon = st_coordinates(centroid)[,1],
    lat = st_coordinates(centroid)[,2]
  )
coords <- st_coordinates(grid_ee_sf$centroid)
colnames(coords) <- c("lon", "lat")
relevant <- cbind(grid_ee_sf |> as.data.frame() |> select(count, starts_with("prob_"), area_bare), coords)
write_csv(relevant, "~/DeVries_WP/dw_4748.csv")
```
```{r}
#| echo: false
#| output: false
#| eval: false

dw_dat <- read_csv("~/DeVries_WP/dw_complete.csv")
# dw_dat <- st_as_sf(dw_dat)
# coords_fun <- function(a) {
#   vals <- a %>% 
#     gsub("list\\(c\\(|\\)", "", .) %>% 
#     strsplit(",") %>% 
#     .[[1]] %>% 
#     as.numeric()
#   lon <- vals[which(vals > 100)]
#   lat <- vals[which(vals <= 100)]
#   return(c(mean(lon), mean(lat)))
# }
# dw_dat <- dw_dat %>%
#   mutate(centroid = st_centroid(geometry)) %>%
#   mutate(
#     lon = st_coordinates(centroid)[,1],
#     lat = st_coordinates(centroid)[,2]
#   )
# coords <- sapply(dw_dat$geometry, coords_fun) |> t()
# colnames(coords) <- c("lon", "lat")
# rownames(coords) <- NULL
dat <- read_csv("~/DeVries_WP/dw_4748.csv")
dat$camp_dist <- sqrt(plgp::distance(cbind(153.5666, -28.8589), dat[, 11 : 12])) |> 
  as.numeric()

set.seed(406)
samp_i <- sample(1 : nrow(dat), replace = F, size = floor(0.7 * nrow(dat)))
write_csv(dat[samp_i, ], "~/DeVries_WP/train.csv")
write_csv(dat[-samp_i, ], "~/DeVries_WP/test.csv")
#eda_i <- sample(1 : length(samp_i), replace = F, size = 500)

#flying fox camp -28.8589, 153.5666

X_train <- read_csv("~/DeVries_WP/train.csv")
Y_train <- X_train$count
X_test <- read_csv("test.csv")
Y_test <- X_test$count
#GGally::ggpairs((dat[samp_i, 1 : 9])[eda_i, 1 : 9])


m1 <- wbart(x.train = X_train[, c(2 : 9, 13)] |> as.matrix(),
            y.train = Y_train,
            x.test = X_test[, c(2 : 9, 13)] |> as.matrix(),
            ntree = 200, power = 2, base = 0.95,
            ndpost = 50000, nskip = 200000, sparse = T)

crps_vals <- crps_sample(Y_test, t(m1$yhat.test))
mosaic::fav_stats(crps_vals) |> round(3) |> knitr::kable()

ggplot(X_test) +
  geom_tile(aes(x = lon, y = lat, fill = crps)) +
  scale_fill_viridis_c()
```
```{r}
#| echo: false
#| output: false
#| eval: false

train <- read_csv("~/DeVries_WP/train.csv")
test <- read_csv("~/DeVries_WP/test.csv")
X_tr <- train[, 2 : 9] |> as.matrix()
X_tes <- test[, 2 : 9] |> as.matrix()

m0 <- wbart(x.train = X_tr, y.train = train$count, x.test = X_tes,
            ndpost = 50000, nskip = 200000)
y_pred <- predict(m0, X_tes)

crps_test <- crps_sample(test$count, t(m0$yhat.test))
```
```{r}
#| echo: false
#| output: false
#| eval: false

ggplot() +
  geom_tile(aes(x = lon, y = lat, fill = count), data = X_train) +
  scale_fill_gradientn(
    colours = c("darkgreen", "forestgreen", "aquamarine2", "cyan"),
    name = "Count",
    trans = "log1p",
    breaks = c(10, 200, 3000, 8600),
    values = scales::rescale(c(10, 200, 3000, 8600))
  ) +
  new_scale_fill() +
  geom_tile(aes(x = lon, y = lat, fill = crps_vals), data = X_test) +
  scale_fill_gradientn(
    colours = viridis::inferno(3),
    values = scales::rescale(c(10, 500, 4000)),
    breaks = c(10, 200, 4000),
    trans = "log",
    name = "CRPS"
  ) +
  theme_minimal() +
  labs(title = "Observed Counts for Training Data and CRPS Values for Testing",
       x = "", y = "")


ggplot() +
  geom_histogram(aes((count) ^ 1 /3, fill = "Observed"),
                 data = X_test, color = "black", binwidth = 40, alpha = 0.5) +
  geom_histogram(aes((m1$yhat.test.mean) ^ 1/3, fill = "Predicted"),
                 color = "black", binwidth = 40, alpha = 0.5)

p0 <- ggplot() +
  geom_histogram(aes(x = count, y = after_stat(count / sum(count)),
                     fill = "Observed"),
                 data = X_test,
                 color = "black", binwidth = 100, alpha = 1) +
  geom_histogram(aes(x = c(m1$yhat.test), y = after_stat(count / sum(count)), fill = "Predicted"),
                 color = "black", binwidth = 100, alpha = 0.3) +
  scale_fill_manual(values = c("Observed" = "gold", "Predicted" = "magenta"), name = "") +
  theme_minimal() +
  labs(title = "Histograms of Posterior Predictive for Testing and Observed Flying Fox Counts",
       x = "Flying Fox Count", y = "Relative Frequency")
p1 <- ggplot() +
    geom_col(aes(x = 0, y = 886, fill = "Observed"),
                  color = "black",
                  alpha = 1, width = 100) +
    geom_histogram(aes((m1$yhat.test.mean[which(m1$yhat.test.mean < 1)]),
                       fill = "Predicted"),
                   color = "black", binwidth = 100, alpha = 0.5) +
    scale_fill_manual(values = c("gold", "magenta"), name = "") +
    theme_minimal() +
    labs(title = "Histograms of Predicted and Observed Flying Fox Counts Below 1",
         x = "Flying Fox Count", y = "Frequency")
ggarrange(p0, p1, labels = c("(A)", "(B)"), ncol = 1)
  

mean(between(X_test$count, colQuantiles(m1$yhat.test, probs = 0.025),
             colQuantiles(m1$yhat.test, probs = 0.975)))

mosaic::fav_stats(crps_vals) |> knitr::kable()


var_dat <- as.data.frame(m1$varprob)
colnames(var_dat)[9] <- "Dist. Flying\nFox Camp"
var_dat$id <- seq_len(nrow(var_dat))
var_long <- var_dat |> 
  pivot_longer(cols = -id, names_to = "variable", values_to = "probability") |> 
  mutate(variable = str_replace(variable, "^prob_", ""),
         variable = str_replace(variable, "^.", ~toupper(.x)))

ggplot(var_long, aes(fill = variable, x = variable, y = probability)) +
  geom_violin(color = "black", trim = FALSE) +
  theme_minimal() +
  labs(y = "Variable Inclusion Probability", title = "Violin Plots of Variable Inclusion Probabilities") +
  scale_fill_viridis_d(name = "")

ggplot() +
  geom_line(aes(x = 200001 : 250000, y = m1$sigma[200001 : 250000]),
            color = "darkgreen") +
  theme_minimal() +
  labs(title = "Trace Plot for Sigma post Burn in", x = "Iteration", y = "Sigma")

p0 <- ggplot(data = data.frame(resid = X_train$count - m1$yhat.train.mean),
       aes(sample = resid)) +
  geom_qq(color = "darkgreen", alpha = 0.5) +
  geom_qq_line() +
  theme_minimal() +
  labs(x = "Theoretical Z-Score", y = "Observed Z-Score",
       title = "Quantile-Quantile Plot for Residuals from Mean BART Predictions on Training")

p1 <- ggplot() +
  geom_point(aes(x = m1$yhat.test.mean,
                 y = scale(X_test$count - m1$yhat.test.mean)), color = "darkgreen") +
  geom_hline(aes(yintercept = 0)) +
  theme_minimal() +
  labs(title = "Standardized Residuals vs Mean Fitted Values for Training",
       x = "Mean Fitted Values", y = "Standardized Residuals")

ggarrange(p0, p1, labels = c("(A)", "(B)"), nrow = 2)
```
```{r}
#| echo: false
#| output: false
#| eval: false

d_mat <- dist(dat[, 11 : 12], diag = T, upper = T) |> as.matrix()
X_fake <- rmvnorm(8, sigma = exp(-d_mat) +
                    diag(1 + .Machine$double.eps, nrow = nrow(d_mat))) |> t()
X_fake_train <- X_fake[samp_i, ]
X_fake_test <- X_fake[-samp_i, ]

m2 <- wbart(x.train = X_fake, y.train = Y_train, x.test = X_test,
            ntree = 200, power = 1.6, base = 0.5,
            ndpost = 60000, nskip = 200000, sparse = T)
crps_fake <- crps_sample(Y_test, t(m2$yhat.test))
mosaic::fav_stats(crps_fake)
```

# Conclusion

Using BART to estimated the flying fox count for three nights of data in Ballina Australia and found some evidence of associations between feeding locations and trees, water, shrubs, grass, flooded vegetation, and man made areas. Poor predictions in high density feeding areas along with violations of modelling assumptions indicate all results are subject to heavy scrutiny. Our model wasn't capable of explaining why so many flying foxes feed in the center of our area of interest, but it appears to have identified some of the characteristics associated with the area. We can't infer causality from any of the potential associations identified as this is an observational study. All results should be taken with a grain of salt.

It's unlikely that our nine covariates are the only factors that influence flying fox feeding locations. Other covariates such as additional land cover characteristics appear to have a non-negligible role as our residuals showed non-white noise. Thinking about land cover, detailed classes likely provide important information. For example, flying foxes are a type of fruit bat, indicating they will be drawn to fruit bearing trees. Exploring additional land cover classes and using models that constrain the response to be non-negative are likely to improve prediction accuracy and precision. Finally, it may also be worthwhile to censor estimated land class probabilities below a certain threshold as low class probabilities may be purely noise. It's possible that land cover alone can provide a reasonable explanation of where flying foxes feed, but a larger variety of classes is certainly required.
\newpage